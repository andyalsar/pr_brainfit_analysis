{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Imports and Configuration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from google.oauth2.credentials import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "import plotly.express as px\n",
    "import configparser\n",
    "import os\n",
    "\n",
    "# Load configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# Google Sheets Configuration\n",
    "SCOPES = config.get('Google', 'SCOPES')\n",
    "GOOGLE_SHEET_ID = config.get('Google', 'GOOGLE_SHEET_ID')\n",
    "GOOGLE_SHEET_RANGE = config.get('Google', 'GOOGLE_SHEET_RANGE')\n",
    "GOOGLE_CREDENTIALS_PATH = config.get('Google', 'GOOGLE_CREDENTIALS_PATH')\n",
    "\n",
    "# Shift Patterns Configuration\n",
    "SHIFT_PATTERNS = {\n",
    "    'standard': {'start': '08:00', 'end': '17:00'},\n",
    "    'early': {'start': '06:00', 'end': '14:00'},\n",
    "    'late': {'start': '14:00', 'end': '22:00'}\n",
    "}\n",
    "\n",
    "GROUP_SHIFTS = {\n",
    "    'Dalmuir': 'standard',\n",
    "    'Kilmalid': 'standard',\n",
    "    'KB3': 'standard'  # Note: This is 'NOPS' in cask data\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Data Loading Functions\n",
    "\n",
    "def load_json_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and parse the JSON biometric data\n",
    "    Args:\n",
    "        file_path: Path to JSON file\n",
    "    Returns:\n",
    "        DataFrame with parsed biometric data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Create lists to store parsed data\n",
    "        parsed_data = []\n",
    "        \n",
    "        # Iterate through each user's data\n",
    "        for user_id, user_data in data.items():\n",
    "            # Extract raw biometric data\n",
    "            raw_data = user_data.get('historic_raw_data', [])\n",
    "            \n",
    "            # Convert to DataFrame format\n",
    "            for record in raw_data:\n",
    "                record['user_id'] = user_id\n",
    "                parsed_data.append(record)\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(parsed_data)\n",
    "        \n",
    "        # Convert timestamp to datetime\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON data: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_google_sheet_data():\n",
    "    \"\"\"\n",
    "    Load user mappings from Google Sheets\n",
    "    Returns:\n",
    "        DataFrame with user mappings\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Set up Google Sheets API\n",
    "        creds = Credentials.from_authorized_user_file(GOOGLE_CREDENTIALS_PATH, SCOPES)\n",
    "        service = build('sheets', 'v4', credentials=creds)\n",
    "        \n",
    "        # Call the Sheets API\n",
    "        sheet = service.spreadsheets()\n",
    "        result = sheet.values().get(\n",
    "            spreadsheetId=GOOGLE_SHEET_ID,\n",
    "            range=GOOGLE_SHEET_RANGE\n",
    "        ).execute()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        values = result.get('values', [])\n",
    "        if not values:\n",
    "            print('No data found in Google Sheet')\n",
    "            return None\n",
    "            \n",
    "        # Assuming first row contains headers\n",
    "        headers = values[0]\n",
    "        data = values[1:]\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Google Sheets data: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_cask_data(file_path):\n",
    "    \"\"\"\n",
    "    Load cask movement data from CSV\n",
    "    Args:\n",
    "        file_path: Path to cask movements CSV\n",
    "    Returns:\n",
    "        DataFrame with cask movements\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert KB3 naming\n",
    "        df['site'] = df['site'].replace('KB3', 'NOPS')\n",
    "        \n",
    "        # Create proper date from month and year\n",
    "        df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "        \n",
    "        # Convert percentage strings to floats\n",
    "        for col in ['receipts_mom_var', 'dispatches_mom_var']:\n",
    "            df[col] = df[col].str.rstrip('%').astype(float) / 100\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cask data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Enhanced validation function with null handling\n",
    "def validate_data_load():\n",
    "    \"\"\"\n",
    "    Test data loading functions and print summaries, including null data analysis\n",
    "    \"\"\"\n",
    "    # Load all data sources\n",
    "    biometric_df = load_json_data('path_to_json')\n",
    "    user_df = load_google_sheet_data()\n",
    "    cask_df = load_cask_data('path_to_cask_csv')\n",
    "    \n",
    "    print(\"\\nBiometric Data Summary:\")\n",
    "    if biometric_df is not None:\n",
    "        print(f\"Number of records: {len(biometric_df)}\")\n",
    "        print(f\"Date range: {biometric_df['timestamp'].min()} to {biometric_df['timestamp'].max()}\")\n",
    "        print(f\"Number of users: {biometric_df['user_id'].nunique()}\")\n",
    "        \n",
    "        # Check for null values\n",
    "        null_counts = biometric_df.isnull().sum()\n",
    "        print(\"\\nNull value counts:\")\n",
    "        print(null_counts[null_counts > 0])\n",
    "        \n",
    "        # Check for gaps in time series\n",
    "        biometric_df['time_diff'] = biometric_df.groupby('user_id')['timestamp'].diff()\n",
    "        large_gaps = biometric_df[biometric_df['time_diff'] > pd.Timedelta(hours=1)]\n",
    "        if not large_gaps.empty:\n",
    "            print(\"\\nFound gaps in data collection (>1 hour):\")\n",
    "            print(f\"Number of gaps: {len(large_gaps)}\")\n",
    "            print(\"Sample gaps:\")\n",
    "            print(large_gaps.head())\n",
    "        \n",
    "    print(\"\\nUser Mapping Summary:\")\n",
    "    if user_df is not None:\n",
    "        print(f\"Number of users: {len(user_df)}\")\n",
    "        print(\"Groups represented:\", user_df['group'].unique())\n",
    "        # Check for missing values in crucial columns\n",
    "        missing_values = user_df[user_df.isnull().any(axis=1)]\n",
    "        if not missing_values.empty:\n",
    "            print(\"\\nWarning: Found incomplete user records:\")\n",
    "            print(missing_values)\n",
    "        \n",
    "    print(\"\\nCask Movement Summary:\")\n",
    "    if cask_df is not None:\n",
    "        print(f\"Date range: {cask_df['date'].min()} to {cask_df['date'].max()}\")\n",
    "        print(\"Sites represented:\", cask_df['site'].unique())\n",
    "        # Check for missing monthly data\n",
    "        expected_months = pd.date_range(cask_df['date'].min(), cask_df['date'].max(), freq='M')\n",
    "        for site in cask_df['site'].unique():\n",
    "            site_data = cask_df[cask_df['site'] == site]\n",
    "            missing_months = [month for month in expected_months \n",
    "                            if month.strftime(\"%Y-%m\") not in site_data['date'].dt.strftime(\"%Y-%m\").values]\n",
    "            if missing_months:\n",
    "                print(f\"\\nMissing months for {site}:\")\n",
    "                print(missing_months)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Data Cleaning and Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Data Cleaning and Processing Functions\n",
    "\n",
    "def clean_biometric_data(df, user_mapping_df):\n",
    "    \"\"\"\n",
    "    Clean and structure biometric data\n",
    "    Args:\n",
    "        df: Raw biometric DataFrame\n",
    "        user_mapping_df: User mapping DataFrame with group information\n",
    "    Returns:\n",
    "        Cleaned DataFrame with working hours filter applied\n",
    "    \"\"\"\n",
    "    # Create copy to avoid modifying original\n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Merge with user mapping to get group information\n",
    "    cleaned_df = cleaned_df.merge(user_mapping_df[['user_id', 'group', 'org']], \n",
    "                                on='user_id', \n",
    "                                how='left')\n",
    "    \n",
    "    # Filter for PR organization\n",
    "    cleaned_df = cleaned_df[cleaned_df['org'] == 'PR']\n",
    "    \n",
    "    # Apply shift patterns\n",
    "    def is_working_hours(row):\n",
    "        shift = SHIFT_PATTERNS[GROUP_SHIFTS[row['group']]]\n",
    "        time = row['timestamp'].time()\n",
    "        start = datetime.strptime(shift['start'], '%H:%M').time()\n",
    "        end = datetime.strptime(shift['end'], '%H:%M').time()\n",
    "        return start <= time <= end\n",
    "    \n",
    "    cleaned_df['is_working_hours'] = cleaned_df.apply(is_working_hours, axis=1)\n",
    "    \n",
    "    # Handle missing values\n",
    "    cleaned_df['heart_rate'] = cleaned_df['heart_rate'].fillna(method='ffill', limit=3)  # Forward fill up to 3 periods\n",
    "    \n",
    "    # Create time-based features\n",
    "    cleaned_df['hour'] = cleaned_df['timestamp'].dt.hour\n",
    "    cleaned_df['day_of_week'] = cleaned_df['timestamp'].dt.day_name()\n",
    "    cleaned_df['week'] = cleaned_df['timestamp'].dt.isocalendar().week\n",
    "    cleaned_df['month'] = cleaned_df['timestamp'].dt.month\n",
    "    \n",
    "    # Flag sparse data periods\n",
    "    def flag_sparse_data(group_df):\n",
    "        expected_records = 12  # Assuming 5-minute intervals\n",
    "        actual_records = len(group_df)\n",
    "        group_df['data_sparse'] = actual_records < expected_records\n",
    "        return group_df\n",
    "    \n",
    "    cleaned_df = cleaned_df.groupby(['user_id', 'timestamp'].dt.floor('H')).apply(flag_sparse_data)\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "def process_cask_data(df):\n",
    "    \"\"\"\n",
    "    Process cask movement data for analysis\n",
    "    Args:\n",
    "        df: Raw cask movement DataFrame\n",
    "    Returns:\n",
    "        Processed DataFrame with additional metrics\n",
    "    \"\"\"\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Calculate efficiency metrics\n",
    "    processed_df['dispatch_receipt_ratio'] = processed_df['dispatches'] / processed_df['receipts']\n",
    "    \n",
    "    # Calculate rolling averages\n",
    "    processed_df['rolling_receipts'] = processed_df.groupby('site')['receipts'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    processed_df['rolling_dispatches'] = processed_df.groupby('site')['dispatches'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "def create_analysis_dataset(biometric_df, cask_df):\n",
    "    \"\"\"\n",
    "    Combine biometric and cask data for analysis\n",
    "    Args:\n",
    "        biometric_df: Cleaned biometric DataFrame\n",
    "        cask_df: Processed cask DataFrame\n",
    "    Returns:\n",
    "        Combined DataFrame ready for analysis\n",
    "    \"\"\"\n",
    "    # Aggregate biometric data to daily level\n",
    "    daily_biometric = biometric_df[biometric_df['is_working_hours']].groupby(\n",
    "        ['group', biometric_df['timestamp'].dt.date]\n",
    "    ).agg({\n",
    "        'heart_rate': ['mean', 'std', 'min', 'max'],\n",
    "        'data_sparse': 'sum',\n",
    "        'user_id': 'nunique'  # Number of users contributing data\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    daily_biometric.columns = ['group', 'date', 'heart_rate_mean', 'heart_rate_std', \n",
    "                             'heart_rate_min', 'heart_rate_max', 'sparse_hours', 'active_users']\n",
    "    \n",
    "    # Convert date to datetime for merging\n",
    "    daily_biometric['date'] = pd.to_datetime(daily_biometric['date'])\n",
    "    \n",
    "    # Merge with cask data\n",
    "    analysis_df = daily_biometric.merge(\n",
    "        cask_df,\n",
    "        left_on=['group', daily_biometric['date'].dt.to_period('M')],\n",
    "        right_on=['site', cask_df['date'].dt.to_period('M')],\n",
    "        how='outer'\n",
    "    )\n",
    "    \n",
    "    return analysis_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Analysis Functions\n",
    "\n",
    "def analyze_group_patterns(analysis_df):\n",
    "    \"\"\"\n",
    "    Analyze patterns within and between groups\n",
    "    Args:\n",
    "        analysis_df: Combined analysis DataFrame\n",
    "    Returns:\n",
    "        Dictionary containing analysis results\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Daily patterns\n",
    "    daily_patterns = analysis_df.groupby(['group', 'hour'])[['heart_rate_mean']].mean()\n",
    "    daily_variation = analysis_df.groupby(['group', 'hour'])[['heart_rate_mean']].std()\n",
    "    \n",
    "    results['daily_patterns'] = {\n",
    "        'mean_patterns': daily_patterns,\n",
    "        'variations': daily_variation\n",
    "    }\n",
    "    \n",
    "    # Weekly patterns\n",
    "    weekly_patterns = analysis_df.groupby(['group', 'day_of_week'])[\n",
    "        ['receipts', 'dispatches', 'heart_rate_mean']\n",
    "    ].mean()\n",
    "    \n",
    "    results['weekly_patterns'] = weekly_patterns\n",
    "    \n",
    "    # Monthly trends\n",
    "    monthly_trends = analysis_df.groupby(['group', 'month'])[\n",
    "        ['receipts', 'dispatches', 'heart_rate_mean']\n",
    "    ].agg(['mean', 'std'])\n",
    "    \n",
    "    results['monthly_trends'] = monthly_trends\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_group_differences(analysis_df):\n",
    "    \"\"\"\n",
    "    Statistical analysis of differences between groups\n",
    "    Args:\n",
    "        analysis_df: Combined analysis DataFrame\n",
    "    Returns:\n",
    "        Dictionary containing statistical test results\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Function for pairwise group comparisons\n",
    "    def pairwise_comparison(data, metric):\n",
    "        groups = data['group'].unique()\n",
    "        comparisons = []\n",
    "        \n",
    "        for i in range(len(groups)):\n",
    "            for j in range(i+1, len(groups)):\n",
    "                group1, group2 = groups[i], groups[j]\n",
    "                stat, pval = stats.ttest_ind(\n",
    "                    data[data['group'] == group1][metric].dropna(),\n",
    "                    data[data['group'] == group2][metric].dropna()\n",
    "                )\n",
    "                comparisons.append({\n",
    "                    'group1': group1,\n",
    "                    'group2': group2,\n",
    "                    'statistic': stat,\n",
    "                    'p_value': pval\n",
    "                })\n",
    "        \n",
    "        return comparisons\n",
    "    \n",
    "    # Compare key metrics\n",
    "    metrics = ['heart_rate_mean', 'receipts', 'dispatches', 'dispatch_receipt_ratio']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        results[f'{metric}_comparisons'] = pairwise_comparison(analysis_df, metric)\n",
    "    \n",
    "    # Identify outlier groups\n",
    "    def identify_outliers(data, metric):\n",
    "        z_scores = stats.zscore(data.groupby('group')[metric].mean())\n",
    "        return pd.Series(z_scores, name='z_score')\n",
    "    \n",
    "    for metric in metrics:\n",
    "        results[f'{metric}_outliers'] = identify_outliers(analysis_df, metric)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_productivity_correlations(analysis_df):\n",
    "    \"\"\"\n",
    "    Analyze correlations between biometric data and productivity metrics\n",
    "    Args:\n",
    "        analysis_df: Combined analysis DataFrame\n",
    "    Returns:\n",
    "        Dictionary containing correlation analyses\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Overall correlations\n",
    "    biometric_cols = ['heart_rate_mean', 'heart_rate_std']\n",
    "    productivity_cols = ['receipts', 'dispatches', 'dispatch_receipt_ratio']\n",
    "    \n",
    "    correlation_matrix = analysis_df[biometric_cols + productivity_cols].corr()\n",
    "    results['overall_correlations'] = correlation_matrix\n",
    "    \n",
    "    # Group-specific correlations\n",
    "    group_correlations = {}\n",
    "    for group in analysis_df['group'].unique():\n",
    "        group_data = analysis_df[analysis_df['group'] == group]\n",
    "        group_correlations[group] = group_data[biometric_cols + productivity_cols].corr()\n",
    "    \n",
    "    results['group_correlations'] = group_correlations\n",
    "    \n",
    "    # Time-lagged correlations\n",
    "    def calculate_lagged_correlation(data, col1, col2, max_lag=3):\n",
    "        lagged_corrs = []\n",
    "        for lag in range(max_lag + 1):\n",
    "            corr = data[col1].corr(data[col2].shift(lag))\n",
    "            lagged_corrs.append({'lag': lag, 'correlation': corr})\n",
    "        return lagged_corrs\n",
    "    \n",
    "    lagged_correlations = {}\n",
    "    for bio_col in biometric_cols:\n",
    "        for prod_col in productivity_cols:\n",
    "            key = f'{bio_col}_vs_{prod_col}'\n",
    "            lagged_correlations[key] = calculate_lagged_correlation(\n",
    "                analysis_df, bio_col, prod_col\n",
    "            )\n",
    "    \n",
    "    results['lagged_correlations'] = lagged_correlations\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_data_quality(analysis_df):\n",
    "    \"\"\"\n",
    "    Analyze data quality and coverage\n",
    "    Args:\n",
    "        analysis_df: Combined analysis DataFrame\n",
    "    Returns:\n",
    "        Dictionary containing data quality metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Data coverage by group\n",
    "    coverage = analysis_df.groupby('group').agg({\n",
    "        'sparse_hours': 'sum',\n",
    "        'active_users': ['mean', 'min', 'max']\n",
    "    })\n",
    "    \n",
    "    results['group_coverage'] = coverage\n",
    "    \n",
    "    # Identify periods of low data quality\n",
    "    low_quality_periods = analysis_df[\n",
    "        (analysis_df['sparse_hours'] > 4) |  # More than 4 hours of sparse data\n",
    "        (analysis_df['active_users'] < 3)     # Less than 3 active users\n",
    "    ]\n",
    "    \n",
    "    results['low_quality_periods'] = low_quality_periods\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_daily_pattern_plots(analysis_results):\n",
    "    \"\"\"\n",
    "    Create visualizations of daily patterns\n",
    "    Args:\n",
    "        analysis_results: Results from analyze_group_patterns\n",
    "    Returns:\n",
    "        Dictionary of plotly figures\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    \n",
    "    # Heart rate patterns throughout the day\n",
    "    daily_patterns = analysis_results['daily_patterns']['mean_patterns'].reset_index()\n",
    "    \n",
    "    fig_heart_rate = px.line(\n",
    "        daily_patterns,\n",
    "        x='hour',\n",
    "        y='heart_rate_mean',\n",
    "        color='group',\n",
    "        title='Average Heart Rate Throughout Working Hours',\n",
    "        labels={'heart_rate_mean': 'Average Heart Rate', 'hour': 'Hour of Day'}\n",
    "    )\n",
    "    fig_heart_rate.update_layout(\n",
    "        hovermode='x unified',\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    figures['daily_heart_rate'] = fig_heart_rate\n",
    "    \n",
    "    return figures\n",
    "\n",
    "def create_productivity_plots(analysis_df):\n",
    "    \"\"\"\n",
    "    Create visualizations of productivity metrics\n",
    "    Args:\n",
    "        analysis_df: Combined analysis DataFrame\n",
    "    Returns:\n",
    "        Dictionary of plotly figures\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    \n",
    "    # Monthly productivity trends\n",
    "    fig_productivity = px.line(\n",
    "        analysis_df,\n",
    "        x='date',\n",
    "        y=['receipts', 'dispatches'],\n",
    "        color='group',\n",
    "        title='Monthly Productivity Metrics',\n",
    "        facet_col='variable',\n",
    "        labels={'value': 'Count', 'date': 'Month'}\n",
    "    )\n",
    "    fig_productivity.update_layout(\n",
    "        hovermode='x unified',\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    figures['monthly_productivity'] = fig_productivity\n",
    "    \n",
    "    # Efficiency ratio plot\n",
    "    fig_efficiency = px.line(\n",
    "        analysis_df,\n",
    "        x='date',\n",
    "        y='dispatch_receipt_ratio',\n",
    "        color='group',\n",
    "        title='Dispatch/Receipt Efficiency Ratio',\n",
    "        labels={'dispatch_receipt_ratio': 'Efficiency Ratio', 'date': 'Month'}\n",
    "    )\n",
    "    \n",
    "    figures['efficiency_ratio'] = fig_efficiency\n",
    "    \n",
    "    return figures\n",
    "\n",
    "def create_correlation_plots(analysis_results):\n",
    "    \"\"\"\n",
    "    Create correlation visualizations\n",
    "    Args:\n",
    "        analysis_results: Results from analyze_productivity_correlations\n",
    "    Returns:\n",
    "        Dictionary of plotly figures\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    \n",
    "    # Overall correlation heatmap\n",
    "    corr_matrix = analysis_results['overall_correlations']\n",
    "    fig_corr = px.imshow(\n",
    "        corr_matrix,\n",
    "        title='Correlation Matrix: Biometrics vs Productivity',\n",
    "        labels=dict(color=\"Correlation\"),\n",
    "        color_continuous_scale='RdBu'\n",
    "    )\n",
    "    \n",
    "    figures['correlation_matrix'] = fig_corr\n",
    "    \n",
    "    # Group-specific correlation comparison\n",
    "    group_corrs = analysis_results['group_correlations']\n",
    "    # Create subplot for each group\n",
    "    from plotly.subplots import make_subplots\n",
    "    fig_group_corr = make_subplots(\n",
    "        rows=1, \n",
    "        cols=len(group_corrs),\n",
    "        subplot_titles=list(group_corrs.keys())\n",
    "    )\n",
    "    \n",
    "    for i, (group, corr) in enumerate(group_corrs.items(), 1):\n",
    "        fig_group_corr.add_trace(\n",
    "            px.imshow(corr).data[0],\n",
    "            row=1, col=i\n",
    "        )\n",
    "    \n",
    "    figures['group_correlations'] = fig_group_corr\n",
    "    \n",
    "    return figures\n",
    "\n",
    "def create_data_quality_plots(analysis_results):\n",
    "    \"\"\"\n",
    "    Create visualizations of data quality metrics\n",
    "    Args:\n",
    "        analysis_results: Results from analyze_data_quality\n",
    "    Returns:\n",
    "        Dictionary of plotly figures\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    \n",
    "    # Data coverage by group\n",
    "    coverage = analysis_results['group_coverage'].reset_index()\n",
    "    fig_coverage = px.bar(\n",
    "        coverage,\n",
    "        x='group',\n",
    "        y=['sparse_hours', 'active_users'],\n",
    "        title='Data Quality Metrics by Group',\n",
    "        barmode='group'\n",
    "    )\n",
    "    \n",
    "    figures['data_quality'] = fig_coverage\n",
    "    \n",
    "    # Timeline of low quality periods\n",
    "    low_quality = analysis_results['low_quality_periods']\n",
    "    fig_quality_timeline = px.scatter(\n",
    "        low_quality,\n",
    "        x='date',\n",
    "        y='group',\n",
    "        size='sparse_hours',\n",
    "        color='active_users',\n",
    "        title='Periods of Low Data Quality',\n",
    "        labels={'sparse_hours': 'Hours of Sparse Data', 'active_users': 'Active Users'}\n",
    "    )\n",
    "    \n",
    "    figures['quality_timeline'] = fig_quality_timeline\n",
    "    \n",
    "    return figures\n",
    "\n",
    "def create_interactive_dashboard(analysis_df, analysis_results):\n",
    "    \"\"\"\n",
    "    Create an interactive dashboard combining key visualizations\n",
    "    Args:\n",
    "        analysis_df: Combined analysis DataFrame\n",
    "        analysis_results: Dictionary of analysis results\n",
    "    Returns:\n",
    "        Plotly figure containing dashboard\n",
    "    \"\"\"\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.graph_objects as go\n",
    "    \n",
    "    # Create main dashboard figure\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Daily Heart Rate Patterns',\n",
    "            'Monthly Productivity',\n",
    "            'Efficiency Ratio',\n",
    "            'Data Quality',\n",
    "            'Correlation Matrix',\n",
    "            'Group Comparison'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add traces from individual plots\n",
    "    daily_patterns = create_daily_pattern_plots(analysis_results)['daily_heart_rate']\n",
    "    productivity_plots = create_productivity_plots(analysis_df)\n",
    "    correlation_plots = create_correlation_plots(analysis_results)\n",
    "    quality_plots = create_data_quality_plots(analysis_results)\n",
    "    \n",
    "    # Combine all plots into dashboard\n",
    "    # (Add specific trace copying code here)\n",
    "    \n",
    "    fig.update_layout(height=1200, width=1600, title_text=\"Group Analysis Dashboard\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution flow for data processing and analysis\n",
    "    \"\"\"\n",
    "    print(\"Starting analysis pipeline...\")\n",
    "    \n",
    "    # 1. Load all data sources\n",
    "    print(\"\\nLoading data...\")\n",
    "    biometric_df = load_json_data('path_to_json')\n",
    "    user_df = load_google_sheet_data()\n",
    "    cask_df = load_cask_data('path_to_cask_csv')\n",
    "    \n",
    "    # Validate data loading\n",
    "    validate_data_load()\n",
    "    \n",
    "    # 2. Clean and process data\n",
    "    print(\"\\nProcessing data...\")\n",
    "    cleaned_biometric = clean_biometric_data(biometric_df, user_df)\n",
    "    processed_cask = process_cask_data(cask_df)\n",
    "    \n",
    "    # 3. Create combined analysis dataset\n",
    "    print(\"\\nCreating analysis dataset...\")\n",
    "    analysis_df = create_analysis_dataset(cleaned_biometric, processed_cask)\n",
    "    \n",
    "    # 4. Run analyses\n",
    "    print(\"\\nRunning analyses...\")\n",
    "    pattern_results = analyze_group_patterns(analysis_df)\n",
    "    difference_results = analyze_group_differences(analysis_df)\n",
    "    correlation_results = analyze_productivity_correlations(analysis_df)\n",
    "    quality_results = analyze_data_quality(analysis_df)\n",
    "    \n",
    "    # Combine all results\n",
    "    analysis_results = {\n",
    "        'patterns': pattern_results,\n",
    "        'differences': difference_results,\n",
    "        'correlations': correlation_results,\n",
    "        'quality': quality_results\n",
    "    }\n",
    "    \n",
    "    # 5. Create visualizations\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    daily_plots = create_daily_pattern_plots(pattern_results)\n",
    "    productivity_plots = create_productivity_plots(analysis_df)\n",
    "    correlation_plots = create_correlation_plots(correlation_results)\n",
    "    quality_plots = create_data_quality_plots(quality_results)\n",
    "    \n",
    "    # 6. Create dashboard\n",
    "    print(\"\\nCreating dashboard...\")\n",
    "    dashboard = create_interactive_dashboard(analysis_df, analysis_results)\n",
    "    \n",
    "    # 7. Save results\n",
    "    print(\"\\nSaving results...\")\n",
    "    \n",
    "    # Save processed data\n",
    "    analysis_df.to_csv('outputs/processed_analysis_data.csv', index=False)\n",
    "    \n",
    "    # Save plots as HTML files\n",
    "    for name, fig in daily_plots.items():\n",
    "        fig.write_html(f'outputs/plots/{name}.html')\n",
    "    for name, fig in productivity_plots.items():\n",
    "        fig.write_html(f'outputs/plots/{name}.html')\n",
    "    for name, fig in correlation_plots.items():\n",
    "        fig.write_html(f'outputs/plots/{name}.html')\n",
    "    for name, fig in quality_plots.items():\n",
    "        fig.write_html(f'outputs/plots/{name}.html')\n",
    "    \n",
    "    # Save dashboard\n",
    "    dashboard.write_html('outputs/dashboard.html')\n",
    "    \n",
    "    # 8. Generate summary report\n",
    "    print(\"\\nGenerating summary report...\")\n",
    "    generate_summary_report(analysis_results, analysis_df)\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Results saved in 'outputs' directory.\")\n",
    "    \n",
    "    return analysis_df, analysis_results, dashboard\n",
    "\n",
    "def generate_summary_report(analysis_results, analysis_df):\n",
    "    \"\"\"\n",
    "    Generate a summary report of key findings\n",
    "    \"\"\"\n",
    "    with open('outputs/summary_report.txt', 'w') as f:\n",
    "        f.write(\"Analysis Summary Report\\n\")\n",
    "        f.write(\"=====================\\n\\n\")\n",
    "        \n",
    "        # Data coverage\n",
    "        f.write(\"Data Coverage:\\n\")\n",
    "        f.write(\"--------------\\n\")\n",
    "        coverage = analysis_results['quality']['group_coverage']\n",
    "        f.write(f\"{coverage.to_string()}\\n\\n\")\n",
    "        \n",
    "        # Key findings\n",
    "        f.write(\"Key Findings:\\n\")\n",
    "        f.write(\"-------------\\n\")\n",
    "        \n",
    "        # Group differences\n",
    "        differences = analysis_results['differences']\n",
    "        for metric, comparisons in differences.items():\n",
    "            if metric.endswith('_comparisons'):\n",
    "                f.write(f\"\\n{metric.replace('_comparisons', '').title()} Comparisons:\\n\")\n",
    "                for comp in comparisons:\n",
    "                    if comp['p_value'] < 0.05:\n",
    "                        f.write(f\"- Significant difference between {comp['group1']} and {comp['group2']}\\n\")\n",
    "        \n",
    "        # Correlations\n",
    "        f.write(\"\\nStrong Correlations:\\n\")\n",
    "        corr_matrix = analysis_results['correlations']['overall_correlations']\n",
    "        strong_corrs = [(i, j, corr_matrix.loc[i,j]) \n",
    "                       for i in corr_matrix.index \n",
    "                       for j in corr_matrix.columns \n",
    "                       if abs(corr_matrix.loc[i,j]) > 0.7 and i != j]\n",
    "        for i, j, corr in strong_corrs:\n",
    "            f.write(f\"- {i} vs {j}: {corr:.2f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Create outputs directory if it doesn't exist\n",
    "        os.makedirs('outputs/plots', exist_ok=True)\n",
    "        \n",
    "        # Run main analysis\n",
    "        analysis_df, analysis_results, dashboard = main()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
