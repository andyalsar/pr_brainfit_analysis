{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pytz\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "from google.oauth2 import service_account \n",
    "from googleapiclient.discovery import build\n",
    "import plotly.express as px\n",
    "import traceback\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'notebook'\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import configuration\n",
    "from config import (\n",
    "    SCOPES,\n",
    "    GOOGLE_SHEET_ID,\n",
    "    GOOGLE_SHEET_RANGE,\n",
    "    GOOGLE_CREDENTIALS_PATH\n",
    ")\n",
    "\n",
    "# Shift Patterns Configuration\n",
    "SHIFT_PATTERNS = {\n",
    "    'standard': {'start': '08:00', 'end': '17:00'},\n",
    "    'early': {'start': '06:00', 'end': '14:00'},\n",
    "    'late': {'start': '14:00', 'end': '22:00'}\n",
    "}\n",
    "\n",
    "GROUP_SHIFTS = {\n",
    "    'dalmuir': 'standard',\n",
    "    'kilmalid': 'standard',\n",
    "    'kb3': 'standard',  # Also handle 'NOPS'\n",
    "    # Add lowercase versions\n",
    "    'DALMUIR': 'standard',\n",
    "    'KILMALID': 'standard',\n",
    "    'KB3': 'standard',\n",
    "    'NOPS': 'standard'\n",
    "}\n",
    "\n",
    "# Enhanced configuration for group mapping\n",
    "GROUP_MAPPING = {\n",
    "    'DALMUIR': ['dalmuir', 'Dalmuir'],\n",
    "    'KILMALID': ['kilmalid', 'Kilmalid'],\n",
    "    'KB3': ['kb3', 'KB3', 'NOPS'],\n",
    "    'SUPERVISORS': ['dalm_sup', 'kilm_sup', 'KB3_sup']\n",
    "}\n",
    "# Update GROUP_SHIFTS with standardized names\n",
    "GROUP_SHIFTS = {\n",
    "    name: 'standard'\n",
    "    for group_list in GROUP_MAPPING.values()\n",
    "    for name in group_list\n",
    "}\n",
    "\n",
    "# Data quality thresholds\n",
    "DATA_QUALITY_THRESHOLDS = {\n",
    "    'min_heart_rate': 40,\n",
    "    'max_heart_rate': 200,\n",
    "    'min_records_per_day': 100,\n",
    "    'max_missing_consecutive': 30  # minutes\n",
    "}\n",
    "\n",
    "# File paths configuration\n",
    "DATA_DIR = Path(os.getenv('DATA_DIR', 'data'))\n",
    "OUTPUT_DIR = Path(os.getenv('OUTPUT_DIR', 'outputs'))\n",
    "\n",
    "# Create directories if they don't exist\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# File paths\n",
    "BIOMETRIC_DATA_PATH = '/Users/andylow/Desktop/Data/prAnalysis/historic_processed_data_for_plotting.json'\n",
    "CASK_DATA_PATH = '/Users/andylow/Desktop/Data/prAnalysis/cask_movements.csv'\n",
    "\n",
    "DATA_QUALITY_CONFIG = {\n",
    "    'sparse_data_threshold': 12,  # minimum records per hour\n",
    "    'forward_fill_limit': 3,      # maximum consecutive values to fill\n",
    "    'min_active_users': 3,        # minimum users for valid analysis\n",
    "    'outlier_threshold': 3.0,     # z-score threshold for outliers\n",
    "    'min_correlation': 0.7,       # threshold for strong correlations\n",
    "    'significant_p_value': 0.05   # threshold for statistical significance\n",
    "}\n",
    "\n",
    "DEFAULT_TIMEZONE = pytz.timezone('Europe/London')  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "SHIFT_PATTERNS: {'standard': {'start': '08:00', 'end': '17:00'}, 'early': {'start': '06:00', 'end': '14:00'}, 'late': {'start': '14:00', 'end': '22:00'}}\n",
      "GROUP_MAPPING: {'DALMUIR': ['dalmuir', 'Dalmuir'], 'KILMALID': ['kilmalid', 'Kilmalid'], 'KB3': ['kb3', 'KB3', 'NOPS'], 'SUPERVISORS': ['dalm_sup', 'kilm_sup', 'KB3_sup']}\n",
      "DATA_QUALITY_THRESHOLDS: {'min_heart_rate': 40, 'max_heart_rate': 200, 'min_records_per_day': 100, 'max_missing_consecutive': 30}\n"
     ]
    }
   ],
   "source": [
    "# Add after imports section\n",
    "print(\"Configuration Summary:\")\n",
    "print(f\"SHIFT_PATTERNS: {SHIFT_PATTERNS}\")\n",
    "print(f\"GROUP_MAPPING: {GROUP_MAPPING}\")\n",
    "print(f\"DATA_QUALITY_THRESHOLDS: {DATA_QUALITY_THRESHOLDS}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Data Loading Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and parse the JSON biometric data with enhanced data extraction\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nAttempting to load JSON data from: {file_path}\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: File not found at {file_path}\")\n",
    "            return None\n",
    "            \n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Initialize lists for different types of data\n",
    "        raw_data_records = []\n",
    "        daily_metrics = []\n",
    "        user_summary = []\n",
    "        \n",
    "        for user_id, user_data in data.items():\n",
    "            try:\n",
    "                # Extract raw time series data\n",
    "                raw_data = user_data.get('historic_raw_data', [])\n",
    "                for record in raw_data:\n",
    "                    record['user_id'] = user_id\n",
    "                    raw_data_records.append(record)\n",
    "                \n",
    "                # Extract daily metrics more comprehensively\n",
    "                daily_data = {\n",
    "                    'rhr': user_data.get('historic_daily_rhr', {}),\n",
    "                    'sleep_scores': user_data.get('historic_daily_sleep_scores', {}),\n",
    "                    'calm_scores': user_data.get('historic_daily_calm_scores', {}),\n",
    "                    'activity_scores': user_data.get('historic_daily_activity_scores', {}),\n",
    "                    'cs': user_data.get('historic_daily_cs', {}),\n",
    "                    'sedentary_minutes': user_data.get('historic_sedentary_minutes_per_day', {}),\n",
    "                    'liss_minutes': user_data.get('historic_liss_minutes_per_day', {}),\n",
    "                    'moderate_minutes': user_data.get('historic_moderate_exercise_minutes_per_day', {}),\n",
    "                    'intense_minutes': user_data.get('historic_intense_exercise_minutes_per_day', {}),\n",
    "                    'consistency_score': user_data.get('historic_consistency_score', {})\n",
    "                }\n",
    "                \n",
    "                # Convert daily metrics to records\n",
    "                dates = set().union(*[d.keys() for d in daily_data.values() if d])\n",
    "                for date in dates:\n",
    "                    daily_metrics.append({\n",
    "                        'user_id': user_id,\n",
    "                        'date': date,\n",
    "                        'rhr': daily_data['rhr'].get(date),\n",
    "                        'sleep_score': daily_data['sleep_scores'].get(date),\n",
    "                        'calm_score': daily_data['calm_scores'].get(date),\n",
    "                        'activity_score': daily_data['activity_scores'].get(date),\n",
    "                        'corescore': daily_data['cs'].get(date),\n",
    "                        'sedentary_minutes': daily_data['sedentary_minutes'].get(date),\n",
    "                        'liss_minutes': daily_data['liss_minutes'].get(date),\n",
    "                        'moderate_minutes': daily_data['moderate_minutes'].get(date),\n",
    "                        'intense_minutes': daily_data['intense_minutes'].get(date),\n",
    "                        'consistency_score': daily_data['consistency_score'].get(date)\n",
    "                    })\n",
    "                \n",
    "                # Extract user summary\n",
    "                user_summary.append({\n",
    "                    'user_id': user_id,\n",
    "                    'name': user_data.get('historic_name'),\n",
    "                    'earliest_date': user_data.get('historic_earliest_date'),\n",
    "                    'latest_date': user_data.get('historic_latest_date'),\n",
    "                    'avg_rhr': user_data.get('historic_resting_heart_rate'),\n",
    "                    'avg_sleep_score': user_data.get('historic_average_sleep_scores'),\n",
    "                    'avg_calm_score': user_data.get('historic_avg_calm_scores'),\n",
    "                    'avg_sedentary_minutes': user_data.get('historic_avg_sedentary_minutes_per_day'),\n",
    "                    'avg_liss_minutes': user_data.get('historic_avg_liss_minutes_per_day'),\n",
    "                    'avg_moderate_minutes': user_data.get('historic_avg_moderate_exercise_minutes_per_day'),\n",
    "                    'avg_intense_minutes': user_data.get('historic_avg_intense_exercise_minutes_per_day'),\n",
    "                    'avg_consistency_score': user_data.get('historic_avg_consistency_score')\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing user {user_id}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Convert to DataFrames with simplified column names\n",
    "        raw_df = pd.DataFrame(raw_data_records)\n",
    "        daily_df = pd.DataFrame(daily_metrics)\n",
    "        user_df = pd.DataFrame(user_summary)\n",
    "        \n",
    "        return {\n",
    "            'raw_data': raw_df,\n",
    "            'daily_metrics': daily_df,\n",
    "            'user_summary': user_df\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading JSON data: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "def load_google_sheet_data():\n",
    "    \"\"\"\n",
    "    Load user mappings from Google Sheets with verbose logging\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\nAttempting to load Google Sheets data...\")\n",
    "        \n",
    "        # Verify credentials file exists\n",
    "        if not os.path.exists(GOOGLE_CREDENTIALS_PATH):\n",
    "            print(f\"Credentials file not found at: {GOOGLE_CREDENTIALS_PATH}\")\n",
    "            return None\n",
    "\n",
    "        print(\"Found credentials file\")\n",
    "        \n",
    "        try:\n",
    "            credentials = service_account.Credentials.from_service_account_file(\n",
    "                GOOGLE_CREDENTIALS_PATH, \n",
    "                scopes=SCOPES\n",
    "            )\n",
    "            print(\"Successfully loaded credentials\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading credentials: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Set up Google Sheets API\n",
    "        try:\n",
    "            service = build('sheets', 'v4', credentials=credentials)\n",
    "            print(\"Successfully built sheets service\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error building sheets service: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Call the Sheets API\n",
    "        try:\n",
    "            sheet = service.spreadsheets()\n",
    "            result = sheet.values().get(\n",
    "                spreadsheetId=GOOGLE_SHEET_ID,\n",
    "                range=GOOGLE_SHEET_RANGE\n",
    "            ).execute()\n",
    "            print(\"Successfully retrieved sheet data\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving sheet data: {e}\")\n",
    "            return None\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        values = result.get('values', [])\n",
    "        if not values:\n",
    "            print('No data found in Google Sheet')\n",
    "            return None\n",
    "            \n",
    "        # Print first few rows of raw data\n",
    "        print(\"\\nFirst few rows of raw sheet data:\")\n",
    "        for row in values[:3]:\n",
    "            print(row)\n",
    "            \n",
    "        headers = [col.strip().lower() for col in values[0]]\n",
    "        data = values[1:]\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "        \n",
    "        # Convert data types\n",
    "        print(\"\\nConverting data types...\")\n",
    "        try:\n",
    "            df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "            df['activity_level'] = pd.to_numeric(df['activity_level'], errors='coerce')\n",
    "            df['progress_this_week'] = pd.to_numeric(df['progress_this_week'], errors='coerce')\n",
    "            df['birthday'] = pd.to_datetime(df['birthday'], errors='coerce')\n",
    "            print(\"Data type conversion successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting data types: {e}\")\n",
    "        \n",
    "        # Check for duplicates\n",
    "        duplicates = df[df['user_id'].duplicated(keep=False)]\n",
    "        if not duplicates.empty:\n",
    "            print(\"\\nFound duplicate user_ids:\")\n",
    "            print(\"Number of duplicates:\", len(duplicates))\n",
    "            print(\"\\nDuplicate entries:\")\n",
    "            for user_id in duplicates['user_id'].unique():\n",
    "                print(f\"\\nEntries for user_id: {user_id}\")\n",
    "                print(duplicates[duplicates['user_id'] == user_id])\n",
    "        \n",
    "        # Handle duplicate user_ids\n",
    "        if df['user_id'].duplicated().any():\n",
    "            print(\"\\nHandling duplicate user_ids...\")\n",
    "            \n",
    "            # Group by user_id and check for exact duplicates\n",
    "            duplicates = df[df['user_id'].duplicated(keep=False)]\n",
    "            \n",
    "            # For exact duplicates, keep only one copy\n",
    "            df = df.drop_duplicates()\n",
    "            \n",
    "            # For different entries with same user_id, keep the most complete record\n",
    "            for user_id in df[df['user_id'].duplicated()]['user_id'].unique():\n",
    "                user_records = df[df['user_id'] == user_id]\n",
    "                # Keep the record with fewer null values\n",
    "                best_record = user_records.isnull().sum(axis=1).idxmin()\n",
    "                df = df[~(df['user_id'] == user_id)] # Remove all records for this user_id\n",
    "                df = pd.concat([df, user_records.loc[[best_record]]])  # Add back the best record\n",
    "            \n",
    "            print(f\"After handling duplicates: {len(df)} unique users\")\n",
    "        \n",
    "        # Print final data info\n",
    "        print(\"\\nFinal DataFrame info:\")\n",
    "        print(df.info())\n",
    "        print(\"\\nNull values summary:\")\n",
    "        print(df.isnull().sum())\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in load_google_sheet_data: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_cask_data(file_path):\n",
    "    \"\"\"\n",
    "    Load cask movement data from CSV with verbose logging\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\nAttempting to load cask data from: {file_path}\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: File not found at {file_path}\")\n",
    "            return None\n",
    "            \n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"Successfully loaded CSV with {len(df)} rows\")\n",
    "        print(\"\\nColumns:\", df.columns.tolist())\n",
    "        print(\"\\nFirst few rows:\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Convert KB3 naming\n",
    "        df['site'] = df['site'].replace('KB3', 'NOPS')\n",
    "        \n",
    "        # Create proper date from month and year\n",
    "        df['date'] = pd.to_datetime(df[['year', 'month']].assign(day=1))\n",
    "        \n",
    "        # Ensure numeric columns are numeric\n",
    "        numeric_cols = ['receipts', 'dispatches', 'receipts_mom_var', 'dispatches_mom_var', \n",
    "                       'ytd_receipts', 'ytd_dispatches']\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                print(f\"\\nConverted {col} to numeric. Sample values:\")\n",
    "                print(df[col].head())\n",
    "        \n",
    "        # Convert percentage values if needed\n",
    "        for col in ['receipts_mom_var', 'dispatches_mom_var']:\n",
    "            if col in df.columns:\n",
    "                if (df[col].abs() > 100).any():\n",
    "                    df[col] = df[col] / 100\n",
    "                    print(f\"\\nConverted {col} to decimal format\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cask data: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "def validate_data_load(biometric_df, user_df, cask_df):\n",
    "    \"\"\"\n",
    "    Test data loading functions and print detailed summaries\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Data Loading Validation ===\")\n",
    "    \n",
    "    print(\"\\nBiometric Data Summary:\")\n",
    "    if biometric_df is not None:\n",
    "        print(f\"Number of records: {len(biometric_df)}\")\n",
    "        print(f\"Columns: {biometric_df.columns.tolist()}\")\n",
    "        print(f\"Number of unique users: {biometric_df['user_id'].nunique()}\")\n",
    "        if 'timestamp' in biometric_df.columns:\n",
    "            print(f\"Date range: {biometric_df['timestamp'].min()} to {biometric_df['timestamp'].max()}\")\n",
    "        print(\"\\nSample data:\")\n",
    "        print(biometric_df.head())\n",
    "    else:\n",
    "        print(\"No biometric data loaded\")\n",
    "        \n",
    "    print(\"\\nUser Mapping Summary:\")\n",
    "    if user_df is not None:\n",
    "        print(f\"Number of users: {len(user_df)}\")\n",
    "        print(f\"Columns: {user_df.columns.tolist()}\")\n",
    "        if 'group' in user_df.columns:\n",
    "            print(\"Groups represented:\", user_df['group'].unique())\n",
    "        print(\"\\nSample data:\")\n",
    "        print(user_df.head())\n",
    "    else:\n",
    "        print(\"No user mapping data loaded\")\n",
    "        \n",
    "    print(\"\\nCask Movement Summary:\")\n",
    "    if cask_df is not None:\n",
    "        print(f\"Number of records: {len(cask_df)}\")\n",
    "        print(f\"Columns: {cask_df.columns.tolist()}\")\n",
    "        if 'site' in cask_df.columns:\n",
    "            print(\"Sites represented:\", cask_df['site'].unique())\n",
    "        print(\"\\nSample data:\")\n",
    "        print(cask_df.head())\n",
    "    else:\n",
    "        print(\"No cask movement data loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting to load JSON data from: /Users/andylow/Desktop/Data/prAnalysis/historic_processed_data_for_plotting.json\n",
      "\n",
      "Sample of loaded raw biometric data:\n",
      "   corescore  heart_rate                  timestamp  \\\n",
      "0        NaN        67.2  2024-08-13T23:00:00+00:00   \n",
      "1        NaN        65.6  2024-08-13T23:05:00+00:00   \n",
      "2        NaN        64.4  2024-08-13T23:10:00+00:00   \n",
      "3        NaN        66.4  2024-08-13T23:15:00+00:00   \n",
      "4        NaN        64.6  2024-08-13T23:20:00+00:00   \n",
      "\n",
      "                                user_id  \n",
      "0  00e0b767-8e3c-481f-aa37-04d329388bc6  \n",
      "1  00e0b767-8e3c-481f-aa37-04d329388bc6  \n",
      "2  00e0b767-8e3c-481f-aa37-04d329388bc6  \n",
      "3  00e0b767-8e3c-481f-aa37-04d329388bc6  \n",
      "4  00e0b767-8e3c-481f-aa37-04d329388bc6  \n",
      "\n",
      "Sample of daily metrics:\n",
      "                                user_id        date   rhr  sleep_score  \\\n",
      "0  00e0b767-8e3c-481f-aa37-04d329388bc6  2024-09-18  None    28.277075   \n",
      "1  00e0b767-8e3c-481f-aa37-04d329388bc6  2024-08-16  None    25.169716   \n",
      "2  00e0b767-8e3c-481f-aa37-04d329388bc6  2024-11-02  None          NaN   \n",
      "3  00e0b767-8e3c-481f-aa37-04d329388bc6  2024-09-02  None          NaN   \n",
      "4  00e0b767-8e3c-481f-aa37-04d329388bc6  2024-10-20  None    12.591557   \n",
      "\n",
      "   calm_score  activity_score  corescore  sedentary_minutes  liss_minutes  \\\n",
      "0   26.773231           38.00  51.101821              150.0         315.0   \n",
      "1   18.875774           28.12  35.062192              440.0         420.0   \n",
      "2   35.041925           38.00  52.545858                0.0         145.0   \n",
      "3   22.173101             NaN        NaN                NaN           NaN   \n",
      "4   23.144268           26.60  30.255135              760.0         160.0   \n",
      "\n",
      "   moderate_minutes  intense_minutes  consistency_score  \n",
      "0             425.0              0.0          87.254902  \n",
      "1              95.0              0.0          93.627451  \n",
      "2             590.0            140.0          85.784314  \n",
      "3               NaN              NaN                NaN  \n",
      "4              35.0              0.0          93.627451  \n",
      "\n",
      "Sample of user summary:\n",
      "                                user_id                name earliest_date  \\\n",
      "0  00e0b767-8e3c-481f-aa37-04d329388bc6         Daniel Jays    2024-08-13   \n",
      "1  02337356-efb4-457f-9fd9-31bd19d3ec8a         Andy Tester    2024-06-01   \n",
      "2  02ae3e21-91b4-4578-b9f0-a9d051ef557f     David McFarlane    2024-08-20   \n",
      "3  0a287a21-705e-4241-ac24-8e146730251b            FCLABS 1    2024-08-17   \n",
      "4  0d3efa2a-5099-4111-9c4b-96eac73fe455  John McConnellogue    2024-06-02   \n",
      "\n",
      "  latest_date avg_rhr  avg_sleep_score  avg_calm_score avg_sedentary_minutes  \\\n",
      "0  2024-11-24    None        23.487997       24.940264                  None   \n",
      "1  2025-02-14    None        21.921347       20.493154                  None   \n",
      "2  2024-11-30    None        17.163472       21.999440                  None   \n",
      "3  2024-08-17    None              NaN       27.250000                  None   \n",
      "4  2024-11-04    None        17.277715       23.149257                  None   \n",
      "\n",
      "  avg_liss_minutes avg_moderate_minutes avg_intense_minutes  \\\n",
      "0             None                 None                None   \n",
      "1             None                 None                None   \n",
      "2             None                 None                None   \n",
      "3             None                 None                None   \n",
      "4             None                 None                None   \n",
      "\n",
      "  avg_consistency_score  \n",
      "0                  None  \n",
      "1                  None  \n",
      "2                  None  \n",
      "3                  None  \n",
      "4                  None  \n",
      "\n",
      "Attempting to load Google Sheets data...\n",
      "Found credentials file\n",
      "Successfully loaded credentials\n",
      "Successfully built sheets service\n",
      "Successfully retrieved sheet data\n",
      "\n",
      "First few rows of raw sheet data:\n",
      "['user_id', 'first_name', 'last_name', 'sex', 'birthday', 'name', 'age', 'org', 'activity_level', 'progress_this_week', 'group']\n",
      "['0a287a21-705e-4241-ac24-8e146730251b', 'FCLABS', '1', '1', '1982-06-15', 'FCLABS 1', '42', 'FCLABS', '2', '34', 'Admin']\n",
      "['80dfd041-ea74-4c6e-878b-270786e02af3', 'FCLABS', '2', '1', '1982-06-15', 'FCLABS 2', '42', 'FCLABS', '33', '40', 'Admin']\n",
      "\n",
      "Converting data types...\n",
      "Data type conversion successful\n",
      "\n",
      "Found duplicate user_ids:\n",
      "Number of duplicates: 6\n",
      "\n",
      "Duplicate entries:\n",
      "\n",
      "Entries for user_id: 74018de9-3cfb-4c38-b8f3-8f4b5ce8c61b\n",
      "                                  user_id first_name  last_name sex birthday  \\\n",
      "118  74018de9-3cfb-4c38-b8f3-8f4b5ce8c61b       Andy  Timezones   1      NaT   \n",
      "125  74018de9-3cfb-4c38-b8f3-8f4b5ce8c61b       Andy  Timezones   1      NaT   \n",
      "\n",
      "               name  age  org  activity_level  progress_this_week group  \n",
      "118  Andy Timezones   40  IND            41.0                -5.0  None  \n",
      "125  Andy Timezones   40  IND            41.0                -5.0  None  \n",
      "\n",
      "Entries for user_id: d15503c3-5a3a-40c6-b2c9-80a167185dd9\n",
      "                                  user_id first_name last_name sex   birthday  \\\n",
      "119  d15503c3-5a3a-40c6-b2c9-80a167185dd9     Connor     Miles   1        NaT   \n",
      "124  d15503c3-5a3a-40c6-b2c9-80a167185dd9     Connor     Miles   1 2001-04-07   \n",
      "\n",
      "             name  age  org  activity_level  progress_this_week group  \n",
      "119  Connor Miles   40  IND            41.0                -5.0  None  \n",
      "124  Connor Miles   23  IND            41.0                -5.0  None  \n",
      "\n",
      "Entries for user_id: 6cb47bbb-5f9f-46d0-9df8-8d31fb2878c1\n",
      "                                  user_id first_name last_name sex   birthday  \\\n",
      "140  6cb47bbb-5f9f-46d0-9df8-8d31fb2878c1      Holly    Conder   2 2001-03-22   \n",
      "141  6cb47bbb-5f9f-46d0-9df8-8d31fb2878c1      Holly    Conder   2 2001-03-22   \n",
      "\n",
      "             name  age  org  activity_level  progress_this_week group  \n",
      "140  Holly Conder   23  IND            41.0                -5.0  None  \n",
      "141  Holly Conder   23  IND            41.0                -5.0  None  \n",
      "\n",
      "Handling duplicate user_ids...\n",
      "After handling duplicates: 143 unique users\n",
      "\n",
      "Final DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 143 entries, 0 to 124\n",
      "Data columns (total 11 columns):\n",
      " #   Column              Non-Null Count  Dtype         \n",
      "---  ------              --------------  -----         \n",
      " 0   user_id             143 non-null    object        \n",
      " 1   first_name          143 non-null    object        \n",
      " 2   last_name           143 non-null    object        \n",
      " 3   sex                 143 non-null    object        \n",
      " 4   birthday            128 non-null    datetime64[ns]\n",
      " 5   name                143 non-null    object        \n",
      " 6   age                 143 non-null    int64         \n",
      " 7   org                 143 non-null    object        \n",
      " 8   activity_level      142 non-null    float64       \n",
      " 9   progress_this_week  142 non-null    float64       \n",
      " 10  group               59 non-null     object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(7)\n",
      "memory usage: 13.4+ KB\n",
      "None\n",
      "\n",
      "Null values summary:\n",
      "user_id                0\n",
      "first_name             0\n",
      "last_name              0\n",
      "sex                    0\n",
      "birthday              15\n",
      "name                   0\n",
      "age                    0\n",
      "org                    0\n",
      "activity_level         1\n",
      "progress_this_week     1\n",
      "group                 84\n",
      "dtype: int64\n",
      "\n",
      "Sample of loaded user data:\n",
      "                                user_id first_name last_name sex   birthday  \\\n",
      "0  0a287a21-705e-4241-ac24-8e146730251b     FCLABS         1   1 1982-06-15   \n",
      "1  80dfd041-ea74-4c6e-878b-270786e02af3     FCLABS         2   1 1982-06-15   \n",
      "2  64525455-7377-4c5b-a46f-f1994bc84e1d     FCLABS         4   1 1982-06-15   \n",
      "3  e15c5483-250d-4db7-9352-02ba29a1694a       Cham         A   1 1982-06-15   \n",
      "4  51c86e59-6a6e-4bc2-be88-40cdd6c98648      Karin  Anderson     1982-06-15   \n",
      "\n",
      "             name  age     org  activity_level  progress_this_week  group  \n",
      "0        FCLABS 1   42  FCLABS             2.0                34.0  Admin  \n",
      "1        FCLABS 2   42  FCLABS            33.0                40.0  Admin  \n",
      "2        FCLABS 4   42  FCLABS            33.0               -12.0  Admin  \n",
      "3          Cham A   42  FCLABS            14.0                31.0   None  \n",
      "4  Karin Anderson   42     ORB            17.0                13.0   None  \n",
      "\n",
      "Attempting to load cask data from: /Users/andylow/Desktop/Data/prAnalysis/cask_movements.csv\n",
      "Successfully loaded CSV with 15 rows\n",
      "\n",
      "Columns: ['month', 'year', 'site', 'receipts', 'receipts_mom_var', 'ytd_receipts', 'dispatches', 'dispatches_mom_var', 'ytd_dispatches']\n",
      "\n",
      "First few rows:\n",
      "   month  year      site  receipts  receipts_mom_var  ytd_receipts  \\\n",
      "0      9  2024   Dalmuir      5077                72         29639   \n",
      "1      9  2024  Kilmalid     11119                53         39652   \n",
      "2      9  2024       KB3     22476               -27        215908   \n",
      "3     10  2024   Dalmuir      6560                29         36199   \n",
      "4     10  2024  Kilmalid      5314               -52         44966   \n",
      "\n",
      "   dispatches  dispatches_mom_var  ytd_dispatches  \n",
      "0        2067                 -68           30138  \n",
      "1        5938                  14           63887  \n",
      "2       22176                 -31          166893  \n",
      "3        4874                 136           35012  \n",
      "4       10744                  81           74631  \n",
      "\n",
      "Converted receipts to numeric. Sample values:\n",
      "0     5077\n",
      "1    11119\n",
      "2    22476\n",
      "3     6560\n",
      "4     5314\n",
      "Name: receipts, dtype: int64\n",
      "\n",
      "Converted dispatches to numeric. Sample values:\n",
      "0     2067\n",
      "1     5938\n",
      "2    22176\n",
      "3     4874\n",
      "4    10744\n",
      "Name: dispatches, dtype: int64\n",
      "\n",
      "Converted receipts_mom_var to numeric. Sample values:\n",
      "0    72\n",
      "1    53\n",
      "2   -27\n",
      "3    29\n",
      "4   -52\n",
      "Name: receipts_mom_var, dtype: int64\n",
      "\n",
      "Converted dispatches_mom_var to numeric. Sample values:\n",
      "0    -68\n",
      "1     14\n",
      "2    -31\n",
      "3    136\n",
      "4     81\n",
      "Name: dispatches_mom_var, dtype: int64\n",
      "\n",
      "Converted ytd_receipts to numeric. Sample values:\n",
      "0     29639\n",
      "1     39652\n",
      "2    215908\n",
      "3     36199\n",
      "4     44966\n",
      "Name: ytd_receipts, dtype: int64\n",
      "\n",
      "Converted ytd_dispatches to numeric. Sample values:\n",
      "0     30138\n",
      "1     63887\n",
      "2    166893\n",
      "3     35012\n",
      "4     74631\n",
      "Name: ytd_dispatches, dtype: int64\n",
      "\n",
      "Converted receipts_mom_var to decimal format\n",
      "\n",
      "Converted dispatches_mom_var to decimal format\n",
      "\n",
      "Sample of loaded cask data:\n",
      "   month  year      site  receipts  receipts_mom_var  ytd_receipts  \\\n",
      "0      9  2024   Dalmuir      5077              0.72         29639   \n",
      "1      9  2024  Kilmalid     11119              0.53         39652   \n",
      "2      9  2024      NOPS     22476             -0.27        215908   \n",
      "3     10  2024   Dalmuir      6560              0.29         36199   \n",
      "4     10  2024  Kilmalid      5314             -0.52         44966   \n",
      "\n",
      "   dispatches  dispatches_mom_var  ytd_dispatches       date  \n",
      "0        2067               -0.68           30138 2024-09-01  \n",
      "1        5938                0.14           63887 2024-09-01  \n",
      "2       22176               -0.31          166893 2024-09-01  \n",
      "3        4874                1.36           35012 2024-10-01  \n",
      "4       10744                0.81           74631 2024-10-01  \n"
     ]
    }
   ],
   "source": [
    "# Add after data loading functions\n",
    "# Test data loading\n",
    "# Test data loading\n",
    "biometric_sample = load_json_data(BIOMETRIC_DATA_PATH)\n",
    "if biometric_sample is not None:\n",
    "    print(\"\\nSample of loaded raw biometric data:\")\n",
    "    print(biometric_sample['raw_data'].head())\n",
    "    \n",
    "    print(\"\\nSample of daily metrics:\")\n",
    "    print(biometric_sample['daily_metrics'].head())\n",
    "    \n",
    "    print(\"\\nSample of user summary:\")\n",
    "    print(biometric_sample['user_summary'].head())\n",
    "\n",
    "user_sample = load_google_sheet_data()\n",
    "if user_sample is not None:\n",
    "    print(\"\\nSample of loaded user data:\")\n",
    "    print(user_sample.head())\n",
    "    \n",
    "cask_sample = load_cask_data(CASK_DATA_PATH)\n",
    "if cask_sample is not None:\n",
    "    print(\"\\nSample of loaded cask data:\")\n",
    "    print(cask_sample.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Data Cleaning and Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Data Cleaning and Processing Functions\n",
    "\n",
    "\n",
    "def convert_to_timezone(df, timezone=DEFAULT_TIMEZONE):\n",
    "    \"\"\"\n",
    "    Convert timestamp column to specified timezone\n",
    "    \"\"\"\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    if df['timestamp'].dt.tz is None:\n",
    "        df['timestamp'] = df['timestamp'].dt.tz_localize('UTC')\n",
    "    df['timestamp'] = df['timestamp'].dt.tz_convert(timezone)\n",
    "    return df\n",
    "\n",
    "def handle_missing_heart_rates(df):\n",
    "    \"\"\"\n",
    "    Handle missing heart rate values through interpolation where appropriate\n",
    "    \"\"\"\n",
    "    print(\"\\nHandling missing heart rate values...\")\n",
    "    \n",
    "    # Group by user and timestamp hour to handle missing values\n",
    "    df = df.copy()\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    \n",
    "    def interpolate_group(group):\n",
    "        # Set timestamp as index for time-based interpolation\n",
    "        group = group.set_index('timestamp')\n",
    "        \n",
    "        # Only interpolate if we have enough good data points\n",
    "        if group['heart_rate'].notna().sum() > len(group) * 0.3:  # At least 30% good data\n",
    "            group['heart_rate'] = group['heart_rate'].interpolate(\n",
    "                method='time', \n",
    "                limit=30  # Max 30 minutes of interpolation\n",
    "            )\n",
    "        \n",
    "        # Reset index to get timestamp back as a column\n",
    "        return group.reset_index()\n",
    "    \n",
    "    # Process each user separately\n",
    "    result_dfs = []\n",
    "    for user_id in df['user_id'].unique():\n",
    "        user_data = df[df['user_id'] == user_id].copy()\n",
    "        user_data = interpolate_group(user_data)\n",
    "        result_dfs.append(user_data)\n",
    "    \n",
    "    # Combine all processed data\n",
    "    df_processed = pd.concat(result_dfs, ignore_index=True)\n",
    "    \n",
    "    # Report remaining missing values\n",
    "    missing_after = df_processed['heart_rate'].isnull().sum()\n",
    "    print(f\"Remaining missing heart rate values: {missing_after}\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "\n",
    "def clean_biometric_data(biometric_dict, user_mapping_df):\n",
    "    try:\n",
    "        print(\"\\nStarting biometric data cleaning...\")\n",
    "        \n",
    "        # Extract the data from the dictionary and validate input\n",
    "        if not isinstance(biometric_dict, dict):\n",
    "            print(\"Error: Expected dictionary input for biometric data\")\n",
    "            return None\n",
    "            \n",
    "        cleaned_df = biometric_dict['raw_data'].copy()\n",
    "        daily_metrics = biometric_dict['daily_metrics'].copy()\n",
    "        \n",
    "        # Standardize group names in user mapping first\n",
    "        user_mapping_df = user_mapping_df.copy()\n",
    "        user_mapping_df['group'] = user_mapping_df['group'].fillna('Unknown')\n",
    "        \n",
    "        def standardize_group(group):\n",
    "            if pd.isna(group):\n",
    "                return 'Unknown'\n",
    "            group = str(group).upper()\n",
    "            \n",
    "            # First check if it's a supervisor group\n",
    "            if '_SUP' in group:\n",
    "                return 'SUPERVISORS'\n",
    "                \n",
    "            # Then check other groups\n",
    "            for std_name, variants in GROUP_MAPPING.items():\n",
    "                if group.upper() in [v.upper() for v in variants]:\n",
    "                    return std_name\n",
    "            return 'Other'\n",
    "        \n",
    "        user_mapping_df['standardized_group'] = user_mapping_df['group'].apply(standardize_group)\n",
    "        \n",
    "        print(f\"Standardized groups: {user_mapping_df['standardized_group'].unique().tolist()}\")\n",
    "        \n",
    "        # Merge with user mapping\n",
    "        cleaned_df = cleaned_df.merge(\n",
    "            user_mapping_df[['user_id', 'standardized_group', 'org']],\n",
    "            on='user_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Merge with daily metrics\n",
    "        cleaned_df = cleaned_df.merge(\n",
    "            daily_metrics,\n",
    "            on='user_id',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Fill any remaining nulls in standardized_group\n",
    "        cleaned_df['standardized_group'] = cleaned_df['standardized_group'].fillna('Unknown')\n",
    "        \n",
    "        print(f\"Data shape after merges: {cleaned_df.shape}\")\n",
    "        \n",
    "        # Add validation for metrics\n",
    "        numeric_columns = [\n",
    "            'rhr', 'sleep_score', 'calm_score', 'activity_score',\n",
    "            'corescore', 'sedentary_minutes', 'liss_minutes',\n",
    "            'moderate_minutes', 'intense_minutes', 'consistency_score'\n",
    "        ]\n",
    "        \n",
    "        for col in numeric_columns:\n",
    "            if col in cleaned_df.columns:\n",
    "                cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors='coerce')\n",
    "                print(f\"Converted {col} to numeric type\")\n",
    "        \n",
    "        # Add activity ratio calculations using new column names\n",
    "        cleaned_df['active_ratio'] = (\n",
    "            cleaned_df['liss_minutes'].fillna(0) + \n",
    "            cleaned_df['moderate_minutes'].fillna(0) + \n",
    "            cleaned_df['intense_minutes'].fillna(0)\n",
    "        ) / (cleaned_df['sedentary_minutes'].fillna(1))  # Add 1 to avoid division by zero\n",
    "        \n",
    "        # Apply working hours filter\n",
    "        def is_working_hours(row):\n",
    "            try:\n",
    "                if pd.isna(row['standardized_group']):\n",
    "                    return False\n",
    "                \n",
    "                shift = SHIFT_PATTERNS.get(GROUP_SHIFTS.get(row['standardized_group']))\n",
    "                if not shift:\n",
    "                    return False\n",
    "                \n",
    "                time = row['timestamp'].time()\n",
    "                start = datetime.strptime(shift['start'], '%H:%M').time()\n",
    "                end = datetime.strptime(shift['end'], '%H:%M').time()\n",
    "                \n",
    "                return start <= time <= end\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {row}\")\n",
    "                print(f\"Error details: {str(e)}\")\n",
    "                return False\n",
    "        \n",
    "        cleaned_df['is_working_hours'] = cleaned_df.apply(is_working_hours, axis=1)\n",
    "        \n",
    "        # Add quality flags\n",
    "        cleaned_df['data_quality'] = 'good'\n",
    "        cleaned_df.loc[cleaned_df['heart_rate'].isnull(), 'data_quality'] = 'missing_heart_rate'\n",
    "        cleaned_df.loc[~cleaned_df['is_working_hours'], 'data_quality'] = 'outside_hours'\n",
    "        \n",
    "        # Add flags for missing wellbeing metrics\n",
    "        for metric in ['sleep_score', 'calm_score', 'corescore']:\n",
    "            if metric in cleaned_df.columns:\n",
    "                cleaned_df.loc[cleaned_df[metric].isnull(), 'data_quality'] = f'missing_{metric}'\n",
    "        \n",
    "        print(\"\\nCleaning summary:\")\n",
    "        print(f\"Total records: {len(cleaned_df)}\")\n",
    "        print(f\"Working hours records: {cleaned_df['is_working_hours'].sum()}\")\n",
    "        print(\"Data quality distribution:\")\n",
    "        print(cleaned_df['data_quality'].value_counts())\n",
    "        \n",
    "        # Handle missing values in the metrics\n",
    "        for col in numeric_columns:\n",
    "            if col in cleaned_df.columns:\n",
    "                cleaned_df[col] = cleaned_df.groupby('user_id')[col].ffill().bfill()\n",
    "                print(f\"Filled missing values in {col}\")\n",
    "        \n",
    "        cleaned_df = handle_missing_heart_rates(cleaned_df)\n",
    "        \n",
    "        return cleaned_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in clean_biometric_data: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "def validate_data_consistency(biometric_df, user_df, cask_df):\n",
    "    \"\"\"\n",
    "    Validate consistency between different data sources\n",
    "    \"\"\"\n",
    "    consistency_report = {\n",
    "        'status': True,\n",
    "        'issues': [],\n",
    "        'warnings': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check user coverage\n",
    "        biometric_users = set(biometric_df['user_id'].unique())\n",
    "        mapped_users = set(user_df['user_id'].unique())\n",
    "        \n",
    "        missing_users = biometric_users - mapped_users\n",
    "        if missing_users:\n",
    "            consistency_report['warnings'].append(\n",
    "                f\"Found {len(missing_users)} users in biometric data without mapping\"\n",
    "            )\n",
    "            \n",
    "        # Check group consistency\n",
    "        cask_sites = set(cask_df['site'].str.upper())\n",
    "        user_groups = set(user_df['group'].dropna().str.upper())\n",
    "        \n",
    "        # Standardize groups for comparison\n",
    "        standardized_sites = {\n",
    "            std_name\n",
    "            for site in cask_sites\n",
    "            for std_name, variants in GROUP_MAPPING.items()\n",
    "            if site in [v.upper() for v in variants]\n",
    "        }\n",
    "        \n",
    "        standardized_groups = {\n",
    "            std_name\n",
    "            for group in user_groups\n",
    "            for std_name, variants in GROUP_MAPPING.items()\n",
    "            if group in [v.upper() for v in variants]\n",
    "        }\n",
    "        \n",
    "        missing_groups = standardized_sites - standardized_groups\n",
    "        if missing_groups:\n",
    "            consistency_report['warnings'].append(\n",
    "                f\"Sites without matching user groups: {missing_groups}\"\n",
    "            )\n",
    "            \n",
    "        extra_groups = standardized_groups - standardized_sites\n",
    "        if extra_groups:\n",
    "            consistency_report['warnings'].append(\n",
    "                f\"User groups without matching sites: {extra_groups}\"\n",
    "            )\n",
    "            \n",
    "    except Exception as e:\n",
    "        consistency_report['status'] = False\n",
    "        consistency_report['issues'].append(f\"Error in consistency validation: {str(e)}\")\n",
    "        \n",
    "    return consistency_report\n",
    "    \n",
    "def validate_group_assignments(user_df, cask_df):\n",
    "    \"\"\"\n",
    "    Validate group assignments between user mapping and cask data\n",
    "    \"\"\"\n",
    "    user_groups = set(user_df['group'].dropna().str.lower())\n",
    "    cask_sites = set(cask_df['site'].str.lower())\n",
    "    \n",
    "    print(\"\\nGroup Assignment Validation:\")\n",
    "    print(f\"User groups: {user_groups}\")\n",
    "    print(f\"Cask sites: {cask_sites}\")\n",
    "    \n",
    "    # Check for mismatches\n",
    "    missing_groups = cask_sites - user_groups\n",
    "    if missing_groups:\n",
    "        print(f\"\\nWarning: Sites without matching user groups: {missing_groups}\")\n",
    "    \n",
    "    extra_groups = user_groups - cask_sites\n",
    "    if extra_groups:\n",
    "        print(f\"\\nWarning: User groups without matching sites: {extra_groups}\")\n",
    "\n",
    "def validate_data_quality(biometric_df, user_df, cask_df):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality validation\n",
    "    \"\"\"\n",
    "    quality_report = {\n",
    "        'status': True,\n",
    "        'issues': [],\n",
    "        'warnings': [],\n",
    "        'metrics': {}\n",
    "    }\n",
    "    \n",
    "    # Biometric data validation\n",
    "    try:\n",
    "        # Check heart rate ranges\n",
    "        invalid_hr = biometric_df[\n",
    "            (biometric_df['heart_rate'].notna()) & \n",
    "            ((biometric_df['heart_rate'] < DATA_QUALITY_THRESHOLDS['min_heart_rate']) |\n",
    "             (biometric_df['heart_rate'] > DATA_QUALITY_THRESHOLDS['max_heart_rate']))\n",
    "        ]\n",
    "        \n",
    "        if not invalid_hr.empty:\n",
    "            quality_report['warnings'].append(\n",
    "                f\"Found {len(invalid_hr)} records with unusual heart rate values\"\n",
    "            )\n",
    "            \n",
    "        # Check data completeness\n",
    "        records_per_user = biometric_df.groupby('user_id').size()\n",
    "        low_data_users = records_per_user[\n",
    "            records_per_user < DATA_QUALITY_THRESHOLDS['min_records_per_day']\n",
    "        ]\n",
    "        \n",
    "        if not low_data_users.empty:\n",
    "            quality_report['warnings'].append(\n",
    "                f\"Found {len(low_data_users)} users with insufficient data\"\n",
    "            )\n",
    "            \n",
    "        # Calculate data coverage\n",
    "        total_users = user_df['user_id'].nunique()\n",
    "        users_with_data = biometric_df['user_id'].nunique()\n",
    "        coverage = (users_with_data / total_users) * 100\n",
    "        \n",
    "        quality_report['metrics']['user_coverage'] = coverage\n",
    "        \n",
    "    except Exception as e:\n",
    "        quality_report['status'] = False\n",
    "        quality_report['issues'].append(f\"Error in biometric validation: {str(e)}\")\n",
    "\n",
    "    return quality_report\n",
    "    \n",
    "def summarize_data_quality(biometric_df, user_df, cask_df):\n",
    "    \"\"\"\n",
    "    Generate comprehensive data quality summary\n",
    "    \"\"\"\n",
    "    print(\"\\nData Quality Summary:\")\n",
    "    \n",
    "    # Biometric data quality\n",
    "    print(\"\\nBiometric Data:\")\n",
    "    print(f\"Total records: {len(biometric_df)}\")\n",
    "    print(f\"Users with data: {biometric_df['user_id'].nunique()}\")\n",
    "    print(\"\\nMissing data rates:\")\n",
    "    for col in ['heart_rate', 'corescore']:\n",
    "        missing_rate = (biometric_df[col].isnull().sum() / len(biometric_df)) * 100\n",
    "        print(f\"{col}: {missing_rate:.1f}% missing\")\n",
    "    \n",
    "    # User mapping quality\n",
    "    print(\"\\nUser Mapping:\")\n",
    "    print(f\"Total users: {len(user_df)}\")\n",
    "    print(\"\\nGroup distribution:\")\n",
    "    print(user_df['group'].value_counts(dropna=False))\n",
    "    \n",
    "    # Cask data quality\n",
    "    print(\"\\nCask Movement Data:\")\n",
    "    print(f\"Total records: {len(cask_df)}\")\n",
    "    print(\"\\nSite distribution:\")\n",
    "    print(cask_df['site'].value_counts())\n",
    "    \n",
    "def process_cask_data(df):\n",
    "    \"\"\"\n",
    "    Process cask movement data for analysis\n",
    "    Args:\n",
    "        df: Raw cask movement DataFrame\n",
    "    Returns:\n",
    "        Processed DataFrame with additional metrics\n",
    "    \"\"\"\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Calculate efficiency metrics\n",
    "    processed_df['dispatch_receipt_ratio'] = processed_df['dispatches'] / processed_df['receipts']\n",
    "    \n",
    "    # Calculate rolling averages\n",
    "    processed_df['rolling_receipts'] = processed_df.groupby('site')['receipts'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    processed_df['rolling_dispatches'] = processed_df.groupby('site')['dispatches'].rolling(window=3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "def create_analysis_dataset(biometric_df, cask_df):\n",
    "    \"\"\"\n",
    "    Combine biometric and cask data for analysis\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\nCreating analysis dataset...\")\n",
    "        \n",
    "        # First, aggregate biometric data to daily level\n",
    "        daily_biometric = biometric_df[biometric_df['is_working_hours']].groupby(\n",
    "            ['standardized_group', biometric_df['timestamp'].dt.date]\n",
    "        ).agg({\n",
    "            'heart_rate': ['mean', 'std', 'min', 'max'],\n",
    "            'user_id': 'nunique'  # Number of users contributing data\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten column names\n",
    "        daily_biometric.columns = ['group', 'date', 'heart_rate_mean', 'heart_rate_std', \n",
    "                                 'heart_rate_min', 'heart_rate_max', 'active_users']\n",
    "        \n",
    "        # Convert date to datetime for merging\n",
    "        daily_biometric['date'] = pd.to_datetime(daily_biometric['date'])\n",
    "        \n",
    "        # Add hour from timestamp\n",
    "        daily_biometric['hour'] = daily_biometric['date'].dt.hour\n",
    "        daily_biometric['day_of_week'] = daily_biometric['date'].dt.day_name()\n",
    "        daily_biometric['week'] = daily_biometric['date'].dt.isocalendar().week\n",
    "        daily_biometric['month'] = daily_biometric['date'].dt.month\n",
    "        \n",
    "        # Prepare cask data for merging\n",
    "        cask_df['date'] = pd.to_datetime(cask_df['date'])\n",
    "        \n",
    "        print(\"\\nBefore merge:\")\n",
    "        print(\"Daily biometric shape:\", daily_biometric.shape)\n",
    "        print(\"Daily biometric date range:\", daily_biometric['date'].min(), \"to\", daily_biometric['date'].max())\n",
    "        print(\"Cask data shape:\", cask_df.shape)\n",
    "        print(\"Cask data date range:\", cask_df['date'].min(), \"to\", cask_df['date'].max())\n",
    "        \n",
    "        # Merge on group and month\n",
    "        analysis_df = pd.merge(\n",
    "            daily_biometric,\n",
    "            cask_df,\n",
    "            left_on=['group', daily_biometric['date'].dt.to_period('M')],\n",
    "            right_on=['site', cask_df['date'].dt.to_period('M')],\n",
    "            how='inner'\n",
    "        )\n",
    "        \n",
    "        return analysis_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_analysis_dataset: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting biometric data cleaning...\n",
      "Standardized groups: ['Other', 'SUPERVISORS', 'DALMUIR', 'KILMALID', 'KB3']\n",
      "Data shape after merges: (218863436, 17)\n",
      "Converted rhr to numeric type\n",
      "Converted sleep_score to numeric type\n",
      "Converted calm_score to numeric type\n",
      "Converted activity_score to numeric type\n",
      "Converted sedentary_minutes to numeric type\n",
      "Converted liss_minutes to numeric type\n",
      "Converted moderate_minutes to numeric type\n",
      "Converted intense_minutes to numeric type\n",
      "Converted consistency_score to numeric type\n"
     ]
    }
   ],
   "source": [
    "# Test cleaning on sample data\n",
    "if biometric_sample is not None and user_sample is not None:\n",
    "    cleaned_bio = clean_biometric_data(biometric_sample, user_sample)\n",
    "    if cleaned_bio is not None:\n",
    "        print(\"\\nSample of cleaned biometric data:\")\n",
    "        print(cleaned_bio.head())\n",
    "        print(\"\\nCleaning summary:\")\n",
    "        print(cleaned_bio['data_quality'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Analysis Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flag_sparse_data(group_df):\n",
    "    \"\"\"\n",
    "    Flag periods with sparse data\n",
    "    Args:\n",
    "        group_df: DataFrame slice for a specific time period\n",
    "    Returns:\n",
    "        DataFrame with sparse data flag added\n",
    "    \"\"\"\n",
    "    expected_records = DATA_QUALITY_CONFIG['sparse_data_threshold']\n",
    "    actual_records = len(group_df)\n",
    "    group_df['data_sparse'] = actual_records < expected_records\n",
    "    return group_df\n",
    "\n",
    "def create_data_quality_plots(quality_results):\n",
    "    \"\"\"\n",
    "    Create visualizations for data quality metrics\n",
    "    Args:\n",
    "        quality_results: Results from analyze_data_quality\n",
    "    Returns:\n",
    "        Dictionary of plotly figures\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    \n",
    "    # Coverage by group\n",
    "    coverage_data = quality_results['group_coverage']\n",
    "    fig_quality = px.bar(\n",
    "        coverage_data.reset_index(),\n",
    "        x='group',\n",
    "        y=['sparse_hours_sum', 'active_users_mean'],\n",
    "        title='Data Quality Metrics by Group',\n",
    "        barmode='group',\n",
    "        labels={\n",
    "            'value': 'Count',\n",
    "            'variable': 'Metric'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    figures['data_quality'] = fig_quality\n",
    "    \n",
    "    # Quality timeline\n",
    "    low_quality = quality_results['low_quality_periods']\n",
    "    fig_timeline = px.scatter(\n",
    "        low_quality,\n",
    "        x='date',\n",
    "        y='group',\n",
    "        size='sparse_hours',\n",
    "        color='active_users',\n",
    "        title='Quality Issues Timeline',\n",
    "        labels={\n",
    "            'sparse_hours': 'Hours with Sparse Data',\n",
    "            'active_users': 'Active Users'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    figures['quality_timeline'] = fig_timeline\n",
    "    \n",
    "    return figures\n",
    "\n",
    "def generate_quality_summary(quality_results):\n",
    "    \"\"\"\n",
    "    Generate HTML summary of data quality analysis\n",
    "    Args:\n",
    "        quality_results: Dictionary containing quality analysis results\n",
    "    Returns:\n",
    "        HTML formatted string\n",
    "    \"\"\"\n",
    "    coverage = quality_results['group_coverage']\n",
    "    low_quality = quality_results['low_quality_periods']\n",
    "    \n",
    "    summary = f\"\"\"\n",
    "    <h4>Data Coverage Summary</h4>\n",
    "    <p>Total groups analyzed: {len(coverage)}</p>\n",
    "    <p>Average active users per group: {coverage['active_users_mean'].mean():.1f}</p>\n",
    "    <p>Total periods with quality issues: {len(low_quality)}</p>\n",
    "    \n",
    "    <h4>Quality Issues Breakdown</h4>\n",
    "    <ul>\n",
    "        <li>Periods with sparse data: {(low_quality['sparse_hours'] > 0).sum()}</li>\n",
    "        <li>Periods with low user count: {(low_quality['active_users'] < DATA_QUALITY_CONFIG['min_active_users']).sum()}</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def generate_group_analysis(pattern_results):\n",
    "    \"\"\"\n",
    "    Generate HTML summary of group pattern analysis\n",
    "    Args:\n",
    "        pattern_results: Dictionary containing pattern analysis results\n",
    "    Returns:\n",
    "        HTML formatted string\n",
    "    \"\"\"\n",
    "    daily_patterns = pattern_results['daily_patterns']\n",
    "    weekly_patterns = pattern_results['weekly_patterns']\n",
    "    \n",
    "    summary = f\"\"\"\n",
    "    <h4>Daily Patterns</h4>\n",
    "    <p>Peak heart rate hours identified for each group:</p>\n",
    "    <ul>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add peak hours for each group\n",
    "    for group in daily_patterns['mean_patterns'].index.get_level_values('group').unique():\n",
    "        group_data = daily_patterns['mean_patterns'].xs(group, level='group')\n",
    "        peak_hour = group_data['heart_rate_mean'].idxmax()\n",
    "        summary += f\"<li>{group}: Hour {peak_hour}</li>\"\n",
    "    \n",
    "    summary += \"\"\"\n",
    "    </ul>\n",
    "    \n",
    "    <h4>Weekly Patterns</h4>\n",
    "    <p>Key observations:</p>\n",
    "    <ul>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add weekly pattern observations\n",
    "    for group in weekly_patterns.index.get_level_values('group').unique():\n",
    "        group_data = weekly_patterns.xs(group, level='group')\n",
    "        peak_day = group_data['heart_rate_mean'].idxmax()\n",
    "        summary += f\"<li>{group}: Peak activity on {peak_day}</li>\"\n",
    "    \n",
    "    summary += \"</ul>\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def analyze_group_patterns(analysis_df):\n",
    "    \"\"\"\n",
    "    Analyze patterns within and between groups\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Define metric groups\n",
    "    wellbeing_metrics = [\n",
    "        'heart_rate_mean', 'historic_daily_sleep_scores', \n",
    "        'historic_daily_calm_scores', 'historic_daily_cs'\n",
    "    ]\n",
    "    \n",
    "    activity_metrics = [\n",
    "        'historic_sedentary_minutes_per_day',\n",
    "        'historic_liss_minutes_per_day',\n",
    "        'historic_moderate_exercise_minutes_per_day',\n",
    "        'historic_intense_exercise_minutes_per_day'\n",
    "    ]\n",
    "    \n",
    "    # Daily patterns for each metric group\n",
    "    for metrics in [wellbeing_metrics, activity_metrics]:\n",
    "        patterns = analysis_df.groupby(['group', 'hour'])[metrics].mean()\n",
    "        variation = analysis_df.groupby(['group', 'hour'])[metrics].std()\n",
    "        \n",
    "        results[f'{metrics[0]}_patterns'] = {\n",
    "            'mean_patterns': patterns,\n",
    "            'variations': variation\n",
    "        }\n",
    "    \n",
    "    # Add weekly aggregations\n",
    "    weekly_patterns = analysis_df.groupby(['group', 'day_of_week'])[\n",
    "        wellbeing_metrics + activity_metrics + ['receipts', 'dispatches']\n",
    "    ].mean()\n",
    "    \n",
    "    results['weekly_patterns'] = weekly_patterns\n",
    "    \n",
    "    return results\n",
    "    \n",
    "    # Function for pairwise group comparisons\n",
    "def analyze_group_differences(analysis_df):\n",
    "    \"\"\"\n",
    "    Statistical analysis of differences between groups\n",
    "    Args:\n",
    "        analysis_df: Combined analysis DataFrame containing biometric and productivity data\n",
    "    Returns:\n",
    "        Dictionary containing statistical test results and outlier analyses\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Define metrics to analyze\n",
    "    metrics = ['heart_rate_mean', 'receipts', 'dispatches', 'dispatch_receipt_ratio']\n",
    "    \n",
    "    def pairwise_comparison(data, metric):\n",
    "        \"\"\"\n",
    "        Perform pairwise statistical tests between groups\n",
    "        Args:\n",
    "            data: DataFrame containing the data\n",
    "            metric: Column name to analyze\n",
    "        Returns:\n",
    "            List of dictionaries containing comparison results\n",
    "        \"\"\"\n",
    "        groups = data['group'].unique()\n",
    "        comparisons = []\n",
    "        \n",
    "        for i in range(len(groups)):\n",
    "            for j in range(i+1, len(groups)):\n",
    "                group1, group2 = groups[i], groups[j]\n",
    "                group1_data = data[data['group'] == group1][metric].dropna()\n",
    "                group2_data = data[data['group'] == group2][metric].dropna()\n",
    "                \n",
    "                # Check for sufficient data\n",
    "                if len(group1_data) < 2 or len(group2_data) < 2:\n",
    "                    continue\n",
    "                \n",
    "                # Check normality\n",
    "                _, p_val1 = stats.normaltest(group1_data)\n",
    "                _, p_val2 = stats.normaltest(group2_data)\n",
    "                \n",
    "                # Choose appropriate test based on normality\n",
    "                if p_val1 > 0.05 and p_val2 > 0.05:\n",
    "                    # Use t-test for normal distributions\n",
    "                    stat, pval = stats.ttest_ind(group1_data, group2_data)\n",
    "                    test_type = 't-test'\n",
    "                else:\n",
    "                    # Use Mann-Whitney U test for non-normal distributions\n",
    "                    stat, pval = stats.mannwhitneyu(group1_data, group2_data, alternative='two-sided')\n",
    "                    test_type = 'Mann-Whitney U'\n",
    "                \n",
    "                # Calculate effect size (Cohen's d)\n",
    "                effect_size = (group1_data.mean() - group2_data.mean()) / np.sqrt(\n",
    "                    ((group1_data.std() ** 2) + (group2_data.std() ** 2)) / 2\n",
    "                )\n",
    "                \n",
    "                comparisons.append({\n",
    "                    'group1': group1,\n",
    "                    'group2': group2,\n",
    "                    'metric': metric,\n",
    "                    'test_type': test_type,\n",
    "                    'statistic': stat,\n",
    "                    'p_value': pval,\n",
    "                    'effect_size': effect_size,\n",
    "                    'significant': pval < DATA_QUALITY_CONFIG['significant_p_value']\n",
    "                })\n",
    "        \n",
    "        return comparisons\n",
    "    \n",
    "    def identify_outliers(data, metric):\n",
    "        \"\"\"\n",
    "        Identify outliers using robust statistical methods\n",
    "        Args:\n",
    "            data: DataFrame containing the data\n",
    "            metric: Column name to analyze\n",
    "        Returns:\n",
    "            DataFrame with outlier flags and z-scores\n",
    "        \"\"\"\n",
    "        group_stats = data.groupby('group')[metric].agg(['median', 'std']).reset_index()\n",
    "        \n",
    "        # Calculate robust z-scores using median and MAD\n",
    "        mad = lambda x: np.median(np.abs(x - np.median(x)))\n",
    "        \n",
    "        outliers = []\n",
    "        for group in data['group'].unique():\n",
    "            group_data = data[data['group'] == group]\n",
    "            median = group_data[metric].median()\n",
    "            mad_value = mad(group_data[metric])\n",
    "            \n",
    "            # Calculate modified z-scores\n",
    "            z_scores = 0.6745 * (group_data[metric] - median) / mad_value\n",
    "            \n",
    "            outliers.extend([{\n",
    "                'group': group,\n",
    "                'timestamp': row['timestamp'],\n",
    "                'value': row[metric],\n",
    "                'z_score': z_score,\n",
    "                'is_outlier': abs(z_score) > DATA_QUALITY_CONFIG['outlier_threshold']\n",
    "            } for row, z_score in zip(group_data.itertuples(), z_scores)])\n",
    "        \n",
    "        return pd.DataFrame(outliers)\n",
    "    \n",
    "    # Perform comparisons for each metric\n",
    "    for metric in metrics:\n",
    "        results[f'{metric}_comparisons'] = pairwise_comparison(analysis_df, metric)\n",
    "        results[f'{metric}_outliers'] = identify_outliers(analysis_df, metric)\n",
    "    \n",
    "    # Add summary statistics\n",
    "    summary_stats = analysis_df.groupby('group')[metrics].agg([\n",
    "        'count', 'mean', 'std', 'min', 'max', 'median'\n",
    "    ]).round(2)\n",
    "    \n",
    "    results['summary_statistics'] = summary_stats\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_productivity_correlations(analysis_df):\n",
    "    \"\"\"\n",
    "    Analyze correlations between biometric data and productivity metrics\n",
    "    Args:\n",
    "        analysis_df: Combined analysis DataFrame\n",
    "    Returns:\n",
    "        Dictionary containing correlation analyses and time-lagged correlations\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Define metric groups\n",
    "    biometric_cols = [\n",
    "        'heart_rate_mean', 'historic_daily_sleep_scores',\n",
    "        'historic_daily_calm_scores', 'historic_daily_cs',\n",
    "        'historic_daily_activity_scores'\n",
    "    ]\n",
    "    \n",
    "    activity_cols = [\n",
    "        'historic_sedentary_minutes_per_day',\n",
    "        'historic_liss_minutes_per_day',\n",
    "        'historic_moderate_exercise_minutes_per_day',\n",
    "        'historic_intense_exercise_minutes_per_day',\n",
    "        'active_ratio'\n",
    "    ]\n",
    "    \n",
    "    productivity_cols = ['receipts', 'dispatches', 'dispatch_receipt_ratio']\n",
    "    \n",
    "    def calculate_correlation_matrix(data, method='pearson'):\n",
    "        \"\"\"\n",
    "        Calculate correlation matrix with confidence intervals\n",
    "        \"\"\"\n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = data[biometric_cols + activity_cols + productivity_cols].corr(method=method)\n",
    "        \n",
    "        # Calculate confidence intervals using Fisher z-transformation\n",
    "        n = len(data)\n",
    "        z_score = stats.norm.ppf(0.975)  # 95% confidence interval\n",
    "        \n",
    "        # Initialize confidence interval matrices\n",
    "        ci_lower = pd.DataFrame(np.zeros_like(corr_matrix), \n",
    "                              index=corr_matrix.index, \n",
    "                              columns=corr_matrix.columns)\n",
    "        ci_upper = ci_lower.copy()\n",
    "        \n",
    "        for i in corr_matrix.index:\n",
    "            for j in corr_matrix.columns:\n",
    "                if i != j:\n",
    "                    r = corr_matrix.loc[i,j]\n",
    "                    z = np.arctanh(r)\n",
    "                    se = 1/np.sqrt(n-3)\n",
    "                    z_lower = z - z_score*se\n",
    "                    z_upper = z + z_score*se\n",
    "                    ci_lower.loc[i,j] = np.tanh(z_lower)\n",
    "                    ci_upper.loc[i,j] = np.tanh(z_upper)\n",
    "        \n",
    "        return {\n",
    "            'correlation_matrix': corr_matrix,\n",
    "            'ci_lower': ci_lower,\n",
    "            'ci_upper': ci_upper\n",
    "        }\n",
    "    \n",
    "    def calculate_lagged_correlation(data, col1, col2, max_lag=3):\n",
    "        \"\"\"\n",
    "        Calculate time-lagged correlations between two variables\n",
    "        \"\"\"\n",
    "        lagged_corrs = []\n",
    "        \n",
    "        for lag in range(max_lag + 1):\n",
    "            # Calculate correlation\n",
    "            corr = data[col1].corr(data[col2].shift(lag))\n",
    "            \n",
    "            # Calculate confidence interval\n",
    "            n = len(data) - lag\n",
    "            z = np.arctanh(corr)\n",
    "            se = 1/np.sqrt(n-3)\n",
    "            z_score = stats.norm.ppf(0.975)\n",
    "            ci_lower = np.tanh(z - z_score*se)\n",
    "            ci_upper = np.tanh(z + z_score*se)\n",
    "            \n",
    "            lagged_corrs.append({\n",
    "                'lag': lag,\n",
    "                'correlation': corr,\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper,\n",
    "                'n_observations': n\n",
    "            })\n",
    "        \n",
    "        return lagged_corrs\n",
    "    \n",
    "    # Calculate overall correlations\n",
    "    results['overall_correlations'] = calculate_correlation_matrix(analysis_df, method='pearson')\n",
    "    results['rank_correlations'] = calculate_correlation_matrix(analysis_df, method='spearman')\n",
    "    \n",
    "    # Calculate group-specific correlations\n",
    "    group_correlations = {}\n",
    "    for group in analysis_df['group'].unique():\n",
    "        group_data = analysis_df[analysis_df['group'] == group]\n",
    "        group_correlations[group] = calculate_correlation_matrix(group_data)\n",
    "    \n",
    "    results['group_correlations'] = group_correlations\n",
    "    \n",
    "    # Calculate lagged correlations for wellbeing metrics\n",
    "    lagged_correlations = {}\n",
    "    for bio_col in biometric_cols + activity_cols:\n",
    "        for prod_col in productivity_cols:\n",
    "            key = f'{bio_col}_vs_{prod_col}'\n",
    "            lagged_correlations[key] = calculate_lagged_correlation(\n",
    "                analysis_df, bio_col, prod_col\n",
    "            )\n",
    "    \n",
    "    results['lagged_correlations'] = lagged_correlations\n",
    "    \n",
    "    # Add statistical significance tests\n",
    "    significance_tests = {}\n",
    "    for metric in biometric_cols + activity_cols:\n",
    "        for prod_metric in productivity_cols:\n",
    "            correlation = stats.pearsonr(\n",
    "                analysis_df[metric].dropna(),\n",
    "                analysis_df[prod_metric].dropna()\n",
    "            )\n",
    "            significance_tests[f'{metric}_vs_{prod_metric}'] = {\n",
    "                'correlation': correlation[0],\n",
    "                'p_value': correlation[1]\n",
    "            }\n",
    "    \n",
    "    results['significance_tests'] = significance_tests\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add after analysis functions\n",
    "# Test analysis on sample data\n",
    "if cleaned_bio is not None and cask_sample is not None:\n",
    "    analysis_df_sample = create_analysis_dataset(cleaned_bio, cask_sample)\n",
    "    if analysis_df_sample is not None:\n",
    "        print(\"\\nSample of analysis dataset:\")\n",
    "        print(analysis_df_sample.head())\n",
    "        \n",
    "        pattern_results = analyze_group_patterns(analysis_df_sample)\n",
    "        print(\"\\nSample of pattern analysis results:\")\n",
    "        print(pattern_results['daily_patterns']['mean_patterns'].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_daily_pattern_plots(analysis_results):\n",
    "    \"\"\"\n",
    "    Create visualizations of daily patterns\n",
    "    Args:\n",
    "        analysis_results: Results from analyze_group_patterns\n",
    "    Returns:\n",
    "        Dictionary of plotly figures\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    \n",
    "    # Heart rate patterns throughout the day\n",
    "    daily_patterns = analysis_results['daily_patterns']['mean_patterns'].reset_index()\n",
    "    \n",
    "    fig_heart_rate = px.line(\n",
    "        daily_patterns,\n",
    "        x='hour',\n",
    "        y='heart_rate_mean',\n",
    "        color='group',\n",
    "        title='Average Heart Rate Throughout Working Hours',\n",
    "        labels={'heart_rate_mean': 'Average Heart Rate', 'hour': 'Hour of Day'}\n",
    "    )\n",
    "    fig_heart_rate.update_layout(\n",
    "        hovermode='x unified',\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    figures['daily_heart_rate'] = fig_heart_rate\n",
    "    \n",
    "    return figures\n",
    "\n",
    "def create_productivity_plots(analysis_df):\n",
    "    \"\"\"\n",
    "    Create visualizations of productivity metrics\n",
    "    Args:\n",
    "        analysis_df: Combined analysis DataFrame\n",
    "    Returns:\n",
    "        Dictionary of plotly figures\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    \n",
    "    # Monthly productivity trends\n",
    "    fig_productivity = px.line(\n",
    "        analysis_df,\n",
    "        x='date',\n",
    "        y=['receipts', 'dispatches'],\n",
    "        color='group',\n",
    "        title='Monthly Productivity Metrics',\n",
    "        facet_col='variable',\n",
    "        labels={'value': 'Count', 'date': 'Month'}\n",
    "    )\n",
    "    fig_productivity.update_layout(\n",
    "        hovermode='x unified',\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    figures['monthly_productivity'] = fig_productivity\n",
    "    \n",
    "    # Efficiency ratio plot\n",
    "    fig_efficiency = px.line(\n",
    "        analysis_df,\n",
    "        x='date',\n",
    "        y='dispatch_receipt_ratio',\n",
    "        color='group',\n",
    "        title='Dispatch/Receipt Efficiency Ratio',\n",
    "        labels={'dispatch_receipt_ratio': 'Efficiency Ratio', 'date': 'Month'}\n",
    "    )\n",
    "    \n",
    "    figures['efficiency_ratio'] = fig_efficiency\n",
    "    \n",
    "    return figures\n",
    "\n",
    "def create_correlation_plots(analysis_results):\n",
    "    \"\"\"\n",
    "    Create correlation visualizations\n",
    "    Args:\n",
    "        analysis_results: Results from analyze_productivity_correlations\n",
    "    Returns:\n",
    "        Dictionary of plotly figures\n",
    "    \"\"\"\n",
    "    figures = {}\n",
    "    \n",
    "    # Overall correlation heatmap\n",
    "    corr_matrix = analysis_results['overall_correlations']\n",
    "    fig_corr = px.imshow(\n",
    "        corr_matrix,\n",
    "        title='Correlation Matrix: Biometrics vs Productivity',\n",
    "        labels=dict(color=\"Correlation\"),\n",
    "        color_continuous_scale='RdBu'\n",
    "    )\n",
    "    \n",
    "    figures['correlation_matrix'] = fig_corr\n",
    "    \n",
    "    # Group-specific correlation comparison\n",
    "    group_corrs = analysis_results['group_correlations']\n",
    "    # Create subplot for each group\n",
    "    from plotly.subplots import make_subplots\n",
    "    fig_group_corr = make_subplots(\n",
    "        rows=1, \n",
    "        cols=len(group_corrs),\n",
    "        subplot_titles=list(group_corrs.keys())\n",
    "    )\n",
    "    \n",
    "    for i, (group, corr) in enumerate(group_corrs.items(), 1):\n",
    "        fig_group_corr.add_trace(\n",
    "            px.imshow(corr).data[0],\n",
    "            row=1, col=i\n",
    "        )\n",
    "    \n",
    "    figures['group_correlations'] = fig_group_corr\n",
    "    \n",
    "    return figures\n",
    "\n",
    "def analyze_data_quality(analysis_df):\n",
    "    \"\"\"\n",
    "    Analyze data quality and coverage\n",
    "    Args:\n",
    "        analysis_df: Combined analysis DataFrame\n",
    "    Returns:\n",
    "        Dictionary containing data quality metrics and analyses\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Data coverage by group\n",
    "    coverage = analysis_df.groupby('group').agg({\n",
    "        'sparse_hours': ['sum', 'mean'],\n",
    "        'active_users': ['mean', 'min', 'max', 'std'],\n",
    "        'heart_rate_mean': ['count', 'isnull', lambda x: x.isnull().sum()/len(x)],\n",
    "        'receipts': ['count', 'isnull', lambda x: x.isnull().sum()/len(x)],\n",
    "        'dispatches': ['count', 'isnull', lambda x: x.isnull().sum()/len(x)]\n",
    "    })\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    coverage.columns = [\n",
    "        f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) else col \n",
    "        for col in coverage.columns\n",
    "    ]\n",
    "    \n",
    "    results['group_coverage'] = coverage\n",
    "    \n",
    "    # Identify periods of low data quality\n",
    "    def identify_low_quality_periods(data):\n",
    "        \"\"\"\n",
    "        Identify periods with data quality issues\n",
    "        Args:\n",
    "            data: DataFrame slice to analyze\n",
    "        Returns:\n",
    "            DataFrame with quality flags\n",
    "        \"\"\"\n",
    "        return pd.DataFrame({\n",
    "            'date': data.index,\n",
    "            'sparse_data': data['sparse_hours'] > DATA_QUALITY_CONFIG['sparse_data_threshold'],\n",
    "            'low_users': data['active_users'] < DATA_QUALITY_CONFIG['min_active_users'],\n",
    "            'missing_biometric': data['heart_rate_mean'].isnull(),\n",
    "            'missing_productivity': data['receipts'].isnull() | data['dispatches'].isnull()\n",
    "        })\n",
    "    \n",
    "    # Analyze quality by time period\n",
    "    daily_quality = analysis_df.resample('D', on='timestamp').apply(identify_low_quality_periods)\n",
    "    weekly_quality = analysis_df.resample('W', on='timestamp').apply(identify_low_quality_periods)\n",
    "    monthly_quality = analysis_df.resample('M', on='timestamp').apply(identify_low_quality_periods)\n",
    "    \n",
    "    results['quality_by_period'] = {\n",
    "        'daily': daily_quality,\n",
    "        'weekly': weekly_quality,\n",
    "        'monthly': monthly_quality\n",
    "    }\n",
    "    \n",
    "    # Analyze data consistency\n",
    "    def check_data_consistency(data):\n",
    "        \"\"\"\n",
    "        Check for data consistency issues\n",
    "        Args:\n",
    "            data: DataFrame to analyze\n",
    "        Returns:\n",
    "            Dictionary of consistency checks\n",
    "        \"\"\"\n",
    "        checks = {\n",
    "            'value_ranges': {\n",
    "                'heart_rate': {\n",
    "                    'min': data['heart_rate_mean'].min(),\n",
    "                    'max': data['heart_rate_mean'].max(),\n",
    "                    'outliers': stats.zscore(data['heart_rate_mean']) > 3\n",
    "                },\n",
    "                'productivity': {\n",
    "                    'min_receipts': data['receipts'].min(),\n",
    "                    'max_receipts': data['receipts'].max(),\n",
    "                    'min_dispatches': data['dispatches'].min(),\n",
    "                    'max_dispatches': data['dispatches'].max(),\n",
    "                    'outliers_receipts': stats.zscore(data['receipts']) > 3,\n",
    "                    'outliers_dispatches': stats.zscore(data['dispatches']) > 3\n",
    "                }\n",
    "            },\n",
    "            'timestamps': {\n",
    "                'gaps': data['timestamp'].diff() > pd.Timedelta(hours=1),\n",
    "                'duplicates': data['timestamp'].duplicated().sum(),\n",
    "                'future_dates': data['timestamp'] > pd.Timestamp.now(),\n",
    "                'past_dates': data['timestamp'] < pd.Timestamp.now() - pd.Timedelta(days=365)\n",
    "            }\n",
    "        }\n",
    "        return checks\n",
    "    \n",
    "    results['data_consistency'] = check_data_consistency(analysis_df)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_quality_assessment_plots(analysis_df, quality_results):\n",
    "    \"\"\"\n",
    "    Create comprehensive data quality visualization plots\n",
    "    Args:\n",
    "        analysis_df: Combined analysis DataFrame\n",
    "        quality_results: Results from analyze_data_quality\n",
    "    Returns:\n",
    "        Dictionary containing plotly figures for data quality assessment\n",
    "    \"\"\"\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    \n",
    "    figures = {}\n",
    "    \n",
    "    # Data coverage heatmap\n",
    "    coverage_data = quality_results['group_coverage']\n",
    "    fig_coverage = px.imshow(\n",
    "        coverage_data,\n",
    "        title='Data Coverage by Group',\n",
    "        labels=dict(x='Metric', y='Group', color='Value'),\n",
    "        aspect='auto'\n",
    "    )\n",
    "    figures['coverage_heatmap'] = fig_coverage\n",
    "    \n",
    "    # Time series of data quality metrics\n",
    "    fig_quality_timeline = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        subplot_titles=('Daily Quality', 'Weekly Quality', 'Monthly Quality')\n",
    "    )\n",
    "    \n",
    "    quality_periods = quality_results['quality_by_period']\n",
    "    \n",
    "    for i, (period, data) in enumerate(quality_periods.items(), 1):\n",
    "        for col in ['sparse_data', 'low_users', 'missing_biometric', 'missing_productivity']:\n",
    "            fig_quality_timeline.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=data.index,\n",
    "                    y=data[col],\n",
    "                    name=f'{period}_{col}',\n",
    "                    mode='lines+markers'\n",
    "                ),\n",
    "                row=i, col=1\n",
    "            )\n",
    "    \n",
    "    fig_quality_timeline.update_layout(\n",
    "        height=900,\n",
    "        title_text=\"Data Quality Timeline\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    figures['quality_timeline'] = fig_quality_timeline\n",
    "    \n",
    "    # Data consistency plots\n",
    "    consistency_data = quality_results['data_consistency']\n",
    "    \n",
    "    # Value range analysis\n",
    "    fig_ranges = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Heart Rate Distribution', 'Heart Rate Outliers',\n",
    "                       'Productivity Distribution', 'Productivity Outliers')\n",
    "    )\n",
    "    \n",
    "    # Heart rate distribution\n",
    "    fig_ranges.add_trace(\n",
    "        go.Histogram(\n",
    "            x=analysis_df['heart_rate_mean'],\n",
    "            name='Heart Rate'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Heart rate outliers\n",
    "    fig_ranges.add_trace(\n",
    "        go.Scatter(\n",
    "            x=analysis_df['timestamp'],\n",
    "            y=analysis_df['heart_rate_mean'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=consistency_data['value_ranges']['heart_rate']['outliers'],\n",
    "                colorscale='Viridis'\n",
    "            ),\n",
    "            name='Heart Rate Outliers'\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Productivity distributions\n",
    "    fig_ranges.add_trace(\n",
    "        go.Histogram(\n",
    "            x=analysis_df['receipts'],\n",
    "            name='Receipts'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig_ranges.add_trace(\n",
    "        go.Histogram(\n",
    "            x=analysis_df['dispatches'],\n",
    "            name='Dispatches'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Productivity outliers\n",
    "    fig_ranges.add_trace(\n",
    "        go.Scatter(\n",
    "            x=analysis_df['timestamp'],\n",
    "            y=analysis_df['receipts'],\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                color=consistency_data['value_ranges']['productivity']['outliers_receipts'],\n",
    "                colorscale='Viridis'\n",
    "            ),\n",
    "            name='Receipt Outliers'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig_ranges.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Data Consistency Analysis\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    figures['consistency_analysis'] = fig_ranges\n",
    "    \n",
    "    return figures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_dashboard(analysis_df, analysis_results):\n",
    "    \"\"\"\n",
    "    Create an interactive dashboard combining key visualizations\n",
    "    \"\"\"\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    \n",
    "    # Create main dashboard figure with subplots\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=2,\n",
    "        subplot_titles=(\n",
    "            'Daily Heart Rate Patterns',\n",
    "            'Sleep & Calm Scores',\n",
    "            'Activity Distribution',\n",
    "            'Monthly Productivity',\n",
    "            'Wellbeing Metrics',\n",
    "            'Exercise Minutes',\n",
    "            'Correlation Matrix',\n",
    "            'Group Performance'\n",
    "        ),\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"heatmap\"}, {\"type\": \"scatter\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]]\n",
    "    )\n",
    "    \n",
    "    # Daily heart rate patterns\n",
    "    daily_patterns = analysis_results['patterns']['heart_rate_mean_patterns']['mean_patterns'].reset_index()\n",
    "    for group in daily_patterns['group'].unique():\n",
    "        group_data = daily_patterns[daily_patterns['group'] == group]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=group_data['hour'],\n",
    "                y=group_data['heart_rate_mean'],\n",
    "                name=f'{group} - Heart Rate',\n",
    "                mode='lines+markers'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Sleep and calm scores\n",
    "    for metric in ['historic_daily_sleep_scores', 'historic_daily_calm_scores']:\n",
    "        for group in analysis_df['group'].unique():\n",
    "            group_data = analysis_df[analysis_df['group'] == group]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=group_data['date'],\n",
    "                    y=group_data[metric],\n",
    "                    name=f'{group} - {metric}',\n",
    "                    mode='lines'\n",
    "                ),\n",
    "                row=1, col=2\n",
    "            )\n",
    "    \n",
    "    # Activity distribution\n",
    "    activity_data = analysis_df.groupby('group')[\n",
    "        ['historic_sedentary_minutes_per_day', 'historic_liss_minutes_per_day',\n",
    "         'historic_moderate_exercise_minutes_per_day', 'historic_intense_exercise_minutes_per_day']\n",
    "    ].mean().reset_index()\n",
    "    \n",
    "    for col in ['historic_sedentary_minutes_per_day', 'historic_liss_minutes_per_day',\n",
    "                'historic_moderate_exercise_minutes_per_day', 'historic_intense_exercise_minutes_per_day']:\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=activity_data['group'],\n",
    "                y=activity_data[col],\n",
    "                name=col.replace('historic_', '').replace('_per_day', '')\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    # Monthly productivity\n",
    "    for group in analysis_df['group'].unique():\n",
    "        group_data = analysis_df[analysis_df['group'] == group]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=group_data['date'],\n",
    "                y=group_data['dispatch_receipt_ratio'],\n",
    "                name=f'{group} - Efficiency',\n",
    "                mode='lines'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Correlation heatmap\n",
    "    corr_matrix = analysis_results['correlations']['overall_correlations']['correlation_matrix']\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=corr_matrix.values,\n",
    "            x=corr_matrix.columns,\n",
    "            y=corr_matrix.index,\n",
    "            colorscale='RdBu',\n",
    "            zmin=-1,\n",
    "            zmax=1\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # Wellbeing metrics over time\n",
    "    for metric in ['historic_daily_cs', 'historic_daily_activity_scores']:\n",
    "        for group in analysis_df['group'].unique():\n",
    "            group_data = analysis_df[analysis_df['group'] == group]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=group_data['date'],\n",
    "                    y=group_data[metric],\n",
    "                    name=f'{group} - {metric}',\n",
    "                    mode='lines'\n",
    "                ),\n",
    "                row=3, col=2\n",
    "            )\n",
    "    \n",
    "    # Exercise minutes trends\n",
    "    exercise_trends = analysis_df.groupby('date')[\n",
    "        ['historic_liss_minutes_per_day', 'historic_moderate_exercise_minutes_per_day',\n",
    "         'historic_intense_exercise_minutes_per_day']\n",
    "    ].mean()\n",
    "    \n",
    "    for col in exercise_trends.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=exercise_trends.index,\n",
    "                y=exercise_trends[col],\n",
    "                name=col.replace('historic_', '').replace('_per_day', ''),\n",
    "                mode='lines'\n",
    "            ),\n",
    "            row=4, col=1\n",
    "        )\n",
    "    \n",
    "    # Group performance comparison\n",
    "    performance_metrics = analysis_df.groupby('group')[\n",
    "        ['historic_daily_cs', 'historic_daily_activity_scores', 'dispatch_receipt_ratio']\n",
    "    ].mean().reset_index()\n",
    "    \n",
    "    for metric in ['historic_daily_cs', 'historic_daily_activity_scores', 'dispatch_receipt_ratio']:\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=performance_metrics['group'],\n",
    "                y=performance_metrics[metric],\n",
    "                name=metric.replace('historic_', '').replace('_', ' '),\n",
    "            ),\n",
    "            row=4, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=1600,  # Increased height to accommodate new plots\n",
    "        width=1600,\n",
    "        title_text=\"Group Analysis Dashboard\",\n",
    "        showlegend=True,\n",
    "        hovermode='closest',\n",
    "        barmode='group'\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Hour of Day\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Group\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=2)\n",
    "    fig.update_xaxes(title_text=\"Metric\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=3, col=2)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=4, col=1)\n",
    "    fig.update_xaxes(title_text=\"Group\", row=4, col=2)\n",
    "    \n",
    "    fig.update_yaxes(title_text=\"Heart Rate\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Score\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Minutes\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Efficiency Ratio\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Metric\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Score\", row=3, col=2)\n",
    "    fig.update_yaxes(title_text=\"Minutes\", row=4, col=1)\n",
    "    fig.update_yaxes(title_text=\"Score\", row=4, col=2)\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_structures(biometric_df, user_df, cask_df):\n",
    "    \"\"\"\n",
    "    Validate the structure and content of input DataFrames\n",
    "    Args:\n",
    "        biometric_df: Biometric data DataFrame\n",
    "        user_df: User mapping DataFrame\n",
    "        cask_df: Cask movement DataFrame\n",
    "    Returns:\n",
    "        Dictionary containing validation results and any issues found\n",
    "    \"\"\"\n",
    "    validation = {\n",
    "        'status': True,\n",
    "        'issues': [],\n",
    "        'warnings': [],\n",
    "        'schema_validation': {}\n",
    "    }\n",
    "    \n",
    "    # Expected columns for each DataFrame\n",
    "    expected_columns = {\n",
    "        'biometric': ['user_id', 'timestamp', 'heart_rate'],\n",
    "        'user': ['user_id', 'group', 'org'],\n",
    "        'cask': ['site', 'date', 'receipts', 'dispatches', 'receipts_mom_var', 'dispatches_mom_var']\n",
    "    }\n",
    "    \n",
    "    # Validate biometric data\n",
    "    if biometric_df is not None:\n",
    "        bio_issues = []\n",
    "        \n",
    "        # Check columns\n",
    "        missing_cols = set(expected_columns['biometric']) - set(biometric_df.columns)\n",
    "        if missing_cols:\n",
    "            bio_issues.append(f\"Missing columns in biometric data: {missing_cols}\")\n",
    "        \n",
    "        # Check data types\n",
    "        if not pd.api.types.is_datetime64_any_dtype(biometric_df['timestamp']):\n",
    "            bio_issues.append(\"timestamp column is not datetime type\")\n",
    "        \n",
    "        if not pd.api.types.is_numeric_dtype(biometric_df['heart_rate']):\n",
    "            bio_issues.append(\"heart_rate column is not numeric type\")\n",
    "        \n",
    "        validation['schema_validation']['biometric'] = bio_issues\n",
    "        \n",
    "    else:\n",
    "        validation['issues'].append(\"Biometric DataFrame is None\")\n",
    "    \n",
    "    # Validate user mapping data\n",
    "    if user_df is not None:\n",
    "        user_issues = []\n",
    "        \n",
    "        # Check columns\n",
    "        missing_cols = set(expected_columns['user']) - set(user_df.columns)\n",
    "        if missing_cols:\n",
    "            user_issues.append(f\"Missing columns in user mapping: {missing_cols}\")\n",
    "        \n",
    "        # Check for duplicate user_ids\n",
    "        if user_df['user_id'].duplicated().any():\n",
    "            user_issues.append(\"Duplicate user_ids found in user mapping\")\n",
    "        \n",
    "        validation['schema_validation']['user'] = user_issues\n",
    "        \n",
    "    else:\n",
    "        validation['issues'].append(\"User mapping DataFrame is None\")\n",
    "    \n",
    "    # Validate cask data\n",
    "    if cask_df is not None:\n",
    "        cask_issues = []\n",
    "        \n",
    "        # Check columns\n",
    "        missing_cols = set(expected_columns['cask']) - set(cask_df.columns)\n",
    "        if missing_cols:\n",
    "            cask_issues.append(f\"Missing columns in cask data: {missing_cols}\")\n",
    "        \n",
    "        # Check numeric columns\n",
    "        numeric_cols = ['receipts', 'dispatches', 'receipts_mom_var', 'dispatches_mom_var']\n",
    "        for col in numeric_cols:\n",
    "            if col in cask_df.columns and not pd.api.types.is_numeric_dtype(cask_df[col]):\n",
    "                cask_issues.append(f\"{col} column is not numeric type\")\n",
    "        \n",
    "        validation['schema_validation']['cask'] = cask_issues\n",
    "        \n",
    "    else:\n",
    "        validation['issues'].append(\"Cask DataFrame is None\")\n",
    "    \n",
    "    # Update overall status\n",
    "    validation['status'] = not (validation['issues'] or any(validation['schema_validation'].values()))\n",
    "    \n",
    "    # Add detailed validation output\n",
    "    print(\"\\nDetailed Validation Results:\")\n",
    "    \n",
    "    if biometric_df is not None:\n",
    "        print(\"\\nBiometric Data Types:\")\n",
    "        print(biometric_df.dtypes)\n",
    "        print(\"\\nBiometric nulls:\")\n",
    "        print(biometric_df.isnull().sum())\n",
    "        print(\"\\nBiometric sample timestamps:\")\n",
    "        print(biometric_df['timestamp'].head())\n",
    "    \n",
    "    if user_df is not None:\n",
    "        print(\"\\nUser Mapping Data Types:\")\n",
    "        print(user_df.dtypes)\n",
    "        print(\"\\nUser mapping nulls:\")\n",
    "        print(user_df.isnull().sum())\n",
    "        if user_df['user_id'].duplicated().any():\n",
    "            duplicates = user_df[user_df['user_id'].duplicated(keep=False)]\n",
    "            print(\"\\nDuplicate user_ids found:\")\n",
    "            for user_id in duplicates['user_id'].unique():\n",
    "                print(f\"\\nEntries for user_id: {user_id}\")\n",
    "                print(duplicates[duplicates['user_id'] == user_id])\n",
    "    \n",
    "    if cask_df is not None:\n",
    "        print(\"\\nCask Data Types:\")\n",
    "        print(cask_df.dtypes)\n",
    "        print(\"\\nCask data nulls:\")\n",
    "        print(cask_df.isnull().sum())\n",
    "    \n",
    "    return validation\n",
    "\n",
    "def generate_analysis_report(analysis_df, analysis_results, output_path):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive analysis report in HTML format\n",
    "    Args:\n",
    "        analysis_df: Combined analysis DataFrame\n",
    "        analysis_results: Dictionary containing all analysis results\n",
    "        output_path: Path to save the HTML report\n",
    "    \"\"\"\n",
    "    import plotly.io as pio\n",
    "    from jinja2 import Template\n",
    "    import json\n",
    "    \n",
    "    # Create report sections\n",
    "    sections = {\n",
    "        'summary': {\n",
    "            'title': 'Executive Summary',\n",
    "            'content': generate_executive_summary(analysis_results)\n",
    "        },\n",
    "        'data_quality': {\n",
    "            'title': 'Data Quality Assessment',\n",
    "            'content': generate_quality_summary(analysis_results['quality'])\n",
    "        },\n",
    "        'group_analysis': {\n",
    "            'title': 'Group Analysis',\n",
    "            'content': generate_group_analysis(analysis_results['patterns'])\n",
    "        },\n",
    "        'correlations': {\n",
    "            'title': 'Correlation Analysis',\n",
    "            'content': generate_correlation_summary(analysis_results['correlations'])\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Convert plots to HTML\n",
    "    plots_html = {}\n",
    "    for name, fig in analysis_results['plots'].items():\n",
    "        plots_html[name] = pio.to_html(fig, full_html=False)\n",
    "    \n",
    "    # Load report template\n",
    "    report_template = \"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Analysis Report</title>\n",
    "        <style>\n",
    "            body { font-family: Arial, sans-serif; margin: 40px; }\n",
    "            .section { margin-bottom: 30px; }\n",
    "            .plot { margin: 20px 0; }\n",
    "            table { border-collapse: collapse; width: 100%; }\n",
    "            th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }\n",
    "            th { background-color: #f2f2f2; }\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Analysis Report</h1>\n",
    "        {% for section in sections.values() %}\n",
    "            <div class=\"section\">\n",
    "                <h2>{{ section.title }}</h2>\n",
    "                {{ section.content }}\n",
    "            </div>\n",
    "        {% endfor %}\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>Visualizations</h2>\n",
    "            {% for name, plot in plots_html.items() %}\n",
    "                <div class=\"plot\">\n",
    "                    <h3>{{ name }}</h3>\n",
    "                    {{ plot }}\n",
    "                </div>\n",
    "            {% endfor %}\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Render template\n",
    "    template = Template(report_template)\n",
    "    report_html = template.render(sections=sections, plots_html=plots_html)\n",
    "    \n",
    "    # Save report\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(report_html)\n",
    "\n",
    "def generate_executive_summary(analysis_results):\n",
    "    \"\"\"\n",
    "    Generate executive summary of analysis results\n",
    "    Args:\n",
    "        analysis_results: Dictionary containing all analysis results\n",
    "    Returns:\n",
    "        HTML string containing executive summary\n",
    "    \"\"\"\n",
    "    # Extract key metrics and findings\n",
    "    quality_metrics = analysis_results['quality']['group_coverage']\n",
    "    correlations = analysis_results['correlations']['overall_correlations']\n",
    "    patterns = analysis_results['patterns']\n",
    "    \n",
    "    # Format summary text\n",
    "    summary = f\"\"\"\n",
    "    <h3>Key Findings</h3>\n",
    "    <ul>\n",
    "        <li>Data Quality: {format_quality_summary(quality_metrics)}</li>\n",
    "        <li>Group Patterns: {format_pattern_summary(patterns)}</li>\n",
    "        <li>Correlations: {format_correlation_summary(correlations)}</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def format_quality_summary(quality_metrics):\n",
    "    \"\"\"Format data quality metrics into readable text\"\"\"\n",
    "    return f\"Analysis covers {len(quality_metrics)} groups with average data completeness of {quality_metrics['heart_rate_mean_count'].mean():.1f}%\"\n",
    "\n",
    "def format_pattern_summary(patterns):\n",
    "    \"\"\"Format pattern analysis into readable text\"\"\"\n",
    "    return \"Key patterns identified in daily and weekly cycles across groups\"\n",
    "\n",
    "def format_correlation_summary(correlations):\n",
    "    \"\"\"Format correlation analysis into readable text\"\"\"\n",
    "    return f\"Strongest correlation observed: {correlations['correlation_matrix'].max().max():.2f}\"\n",
    "\n",
    "def validate_data_completeness(biometric_df, cask_df):\n",
    "    \"\"\"Validate data completeness before merging\"\"\"\n",
    "    completeness = {\n",
    "        'status': True,\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_bio = ['standardized_group', 'timestamp', 'heart_rate', 'is_working_hours']\n",
    "    required_cask = ['site', 'date', 'receipts', 'dispatches']\n",
    "    \n",
    "    missing_bio = [col for col in required_bio if col not in biometric_df.columns]\n",
    "    missing_cask = [col for col in required_cask if col not in cask_df.columns]\n",
    "    \n",
    "    if missing_bio:\n",
    "        completeness['status'] = False\n",
    "        completeness['issues'].append(f\"Missing biometric columns: {missing_bio}\")\n",
    "    \n",
    "    if missing_cask:\n",
    "        completeness['status'] = False\n",
    "        completeness['issues'].append(f\"Missing cask columns: {missing_cask}\")\n",
    "    \n",
    "    return completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pattern_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[188], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Add after visualization functions\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Test visualizations on sample data\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mif\u001b[39;00m analysis_df_sample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m pattern_results \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     get_ipython()\u001b[39m.\u001b[39mrun_line_magic(\u001b[39m'\u001b[39m\u001b[39mmatplotlib\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39minline  # Enable inline plotting\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pattern_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Add after visualization functions\n",
    "# Test visualizations on sample data\n",
    "if analysis_df_sample is not None and pattern_results is not None:\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline  # Enable inline plotting\n",
    "    \n",
    "    daily_plots = create_daily_pattern_plots(pattern_results)\n",
    "    if daily_plots:\n",
    "        for name, fig in daily_plots.items():\n",
    "            fig.show()\n",
    "    \n",
    "    productivity_plots = create_productivity_plots(analysis_df_sample)\n",
    "    if productivity_plots:\n",
    "        for name, fig in productivity_plots.items():\n",
    "            fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Main Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_data_shape(df, step_name):\n",
    "    \"\"\"Log the shape and columns of a DataFrame\"\"\"\n",
    "    print(f\"\\n{step_name}:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(\"Sample rows:\")\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_group_matching(biometric_df, cask_df):\n",
    "    \"\"\"\n",
    "    Validate group names match between datasets\n",
    "    \"\"\"\n",
    "    try:\n",
    "        bio_groups = set(biometric_df['standardized_group'].dropna().unique())\n",
    "        cask_sites = set(cask_df['site'].unique())\n",
    "        \n",
    "        print(\"\\nGroup matching validation:\")\n",
    "        print(f\"Biometric groups: {bio_groups}\")\n",
    "        print(f\"Cask sites: {cask_sites}\")\n",
    "        \n",
    "        mismatches = bio_groups.symmetric_difference(cask_sites)\n",
    "        if mismatches:\n",
    "            print(f\"\\nWarning: Group mismatches found: {mismatches}\")\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in validate_group_matching: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return False\n",
    "\n",
    "def validate_data_completeness(biometric_df, cask_df):\n",
    "    \"\"\"\n",
    "    Validate data completeness before merging\n",
    "    \"\"\"\n",
    "    completeness = {\n",
    "        'status': True,\n",
    "        'issues': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check for required columns\n",
    "        required_bio = ['standardized_group', 'timestamp', 'heart_rate', 'is_working_hours']\n",
    "        required_cask = ['site', 'date', 'receipts', 'dispatches']\n",
    "        \n",
    "        missing_bio = [col for col in required_bio if col not in biometric_df.columns]\n",
    "        missing_cask = [col for col in required_cask if col not in cask_df.columns]\n",
    "        \n",
    "        if missing_bio:\n",
    "            completeness['status'] = False\n",
    "            completeness['issues'].append(f\"Missing biometric columns: {missing_bio}\")\n",
    "        \n",
    "        if missing_cask:\n",
    "            completeness['status'] = False\n",
    "            completeness['issues'].append(f\"Missing cask columns: {missing_cask}\")\n",
    "        \n",
    "        # Check for null values in key columns\n",
    "        if biometric_df['standardized_group'].isnull().any():\n",
    "            completeness['issues'].append(\n",
    "                f\"Found {biometric_df['standardized_group'].isnull().sum()} null values in standardized_group\"\n",
    "            )\n",
    "        \n",
    "        if biometric_df['heart_rate'].isnull().any():\n",
    "            completeness['issues'].append(\n",
    "                f\"Found {biometric_df['heart_rate'].isnull().sum()} null values in heart_rate\"\n",
    "            )\n",
    "            \n",
    "        # Check date ranges\n",
    "        bio_date_range = pd.date_range(\n",
    "            start=biometric_df['timestamp'].min(),\n",
    "            end=biometric_df['timestamp'].max(),\n",
    "            freq='D'\n",
    "        )\n",
    "        cask_date_range = pd.date_range(\n",
    "            start=cask_df['date'].min(),\n",
    "            end=cask_df['date'].max(),\n",
    "            freq='D'\n",
    "        )\n",
    "        \n",
    "        date_overlap = set(bio_date_range).intersection(set(cask_date_range))\n",
    "        if not date_overlap:\n",
    "            completeness['status'] = False\n",
    "            completeness['issues'].append(\"No date overlap between biometric and cask data\")\n",
    "        \n",
    "        bio_dates = pd.to_datetime(biometric_df['timestamp']).dt.date\n",
    "        cask_dates = pd.to_datetime(cask_df['date']).dt.date\n",
    "        \n",
    "        print(\"\\nDate ranges:\")\n",
    "        print(f\"Biometric data: {bio_dates.min()} to {bio_dates.max()}\")\n",
    "        print(f\"Cask data: {cask_dates.min()} to {cask_dates.max()}\")\n",
    "        \n",
    "        # Check for any overlap\n",
    "        bio_date_set = set(bio_dates)\n",
    "        cask_date_set = set(cask_dates)\n",
    "        date_overlap = bio_date_set.intersection(cask_date_set)\n",
    "        \n",
    "        if not date_overlap:\n",
    "            completeness['status'] = False\n",
    "            completeness['issues'].append(\n",
    "                \"No date overlap between biometric and cask data\\n\" +\n",
    "                f\"Biometric: {bio_dates.min()} to {bio_dates.max()}\\n\" +\n",
    "                f\"Cask: {cask_dates.min()} to {cask_dates.max()}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"\\nFound {len(date_overlap)} days of overlapping data\")\n",
    "            \n",
    "        return completeness\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in validate_data_completeness: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        completeness['status'] = False\n",
    "        completeness['issues'].append(f\"Error during validation: {str(e)}\")\n",
    "        return completeness\n",
    "\n",
    "def summarize_validation_results(biometric_df, cask_df):\n",
    "    \"\"\"\n",
    "    Print summary statistics of the validation\n",
    "    \"\"\"\n",
    "    print(\"\\nValidation Summary:\")\n",
    "    \n",
    "    print(\"\\nBiometric Data:\")\n",
    "    print(f\"Total records: {len(biometric_df)}\")\n",
    "    print(f\"Date range: {biometric_df['timestamp'].min()} to {biometric_df['timestamp'].max()}\")\n",
    "    print(\"\\nGroup distribution:\")\n",
    "    print(biometric_df['standardized_group'].value_counts(dropna=False))\n",
    "    \n",
    "    print(\"\\nCask Data:\")\n",
    "    print(f\"Total records: {len(cask_df)}\")\n",
    "    print(f\"Date range: {cask_df['date'].min()} to {cask_df['date'].max()}\")\n",
    "    print(\"\\nSite distribution:\")\n",
    "    print(cask_df['site'].value_counts())\n",
    "\n",
    "def filter_to_valid_dates(biometric_df, cask_df):\n",
    "    \"\"\"\n",
    "    Filter both datasets to overlapping date ranges\n",
    "    \"\"\"\n",
    "    print(\"\\nFiltering to overlapping date ranges...\")\n",
    "    \n",
    "    # Convert timestamps to date\n",
    "    biometric_df = biometric_df.copy()\n",
    "    cask_df = cask_df.copy()\n",
    "    \n",
    "    biometric_df['date'] = pd.to_datetime(biometric_df['timestamp']).dt.date\n",
    "    cask_df['date'] = pd.to_datetime(cask_df['date']).dt.date\n",
    "    \n",
    "    # Get overlapping date range\n",
    "    start_date = max(biometric_df['date'].min(), cask_df['date'].min())\n",
    "    end_date = min(biometric_df['date'].max(), cask_df['date'].max())\n",
    "    \n",
    "    print(f\"Filtering to date range: {start_date} to {end_date}\")\n",
    "    \n",
    "    # Filter both datasets\n",
    "    bio_filtered = biometric_df[\n",
    "        (biometric_df['date'] >= start_date) & \n",
    "        (biometric_df['date'] <= end_date)\n",
    "    ]\n",
    "    cask_filtered = cask_df[\n",
    "        (cask_df['date'] >= start_date) & \n",
    "        (cask_df['date'] <= end_date)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Filtered biometric records: {len(bio_filtered)} (was {len(biometric_df)})\")\n",
    "    print(f\"Filtered cask records: {len(cask_filtered)} (was {len(cask_df)})\")\n",
    "    \n",
    "    return bio_filtered, cask_filtered\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Enhanced main execution flow with comprehensive error handling and validation\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Starting analysis pipeline...\")\n",
    "        \n",
    "        # 1. Load Data\n",
    "        print(\"\\n=== Loading Data Sources ===\")\n",
    "        biometric_df = load_json_data(BIOMETRIC_DATA_PATH)\n",
    "        user_df = load_google_sheet_data()\n",
    "        cask_df = load_cask_data(CASK_DATA_PATH)\n",
    "        \n",
    "        if any(df is None for df in [biometric_df, user_df, cask_df]):\n",
    "            print(\"Error: Failed to load one or more data sources\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # 2. Validate Data Structures\n",
    "        print(\"\\n=== Validating Data Structures ===\")\n",
    "        structure_validation = validate_data_structures(biometric_df, user_df, cask_df)\n",
    "        if not structure_validation['status']:\n",
    "            print(\"Data structure validation failed:\")\n",
    "            for issue in structure_validation['issues']:\n",
    "                print(f\"- {issue}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # 3. Clean and Process Data\n",
    "        print(\"\\n=== Processing Data ===\")\n",
    "        \n",
    "        # Clean biometric data\n",
    "        cleaned_biometric = clean_biometric_data(biometric_df, user_df)\n",
    "        if cleaned_biometric is None:\n",
    "            print(\"Error: Failed to clean biometric data\")\n",
    "            return None, None, None\n",
    "        log_data_shape(cleaned_biometric, \"Cleaned biometric data\")\n",
    "        \n",
    "        # Process cask data\n",
    "        processed_cask = process_cask_data(cask_df)\n",
    "        if processed_cask is None:\n",
    "            print(\"Error: Failed to process cask data\")\n",
    "            return None, None, None\n",
    "        log_data_shape(processed_cask, \"Processed cask data\")\n",
    "        \n",
    "        # 4. Validate Data Quality\n",
    "        print(\"\\n=== Validating Data Quality ===\")\n",
    "        quality_report = validate_data_quality(cleaned_biometric, user_df, processed_cask)\n",
    "        if not quality_report['status']:\n",
    "            print(\"Data quality validation failed:\")\n",
    "            for issue in quality_report['issues']:\n",
    "                print(f\"- {issue}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # 5. Check Data Consistency\n",
    "        print(\"\\n=== Checking Data Consistency ===\")\n",
    "        consistency_report = validate_data_consistency(cleaned_biometric, user_df, processed_cask)\n",
    "        if not consistency_report['status']:\n",
    "            print(\"Data consistency validation failed:\")\n",
    "            for issue in consistency_report['issues']:\n",
    "                print(f\"- {issue}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # 6. Validate Data Completeness\n",
    "        print(\"\\n=== Validating Data Completeness ===\")\n",
    "        completeness_check = validate_data_completeness(cleaned_biometric, processed_cask)\n",
    "        if not completeness_check['status']:\n",
    "            print(\"Data completeness check failed:\")\n",
    "            for issue in completeness_check['issues']:\n",
    "                print(f\"- {issue}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # 7. Validate Group Matching\n",
    "        print(\"\\n=== Validating Group Matching ===\")\n",
    "        if not validate_group_matching(cleaned_biometric, processed_cask):\n",
    "            print(\"Warning: Proceeding with analysis despite group mismatches\")\n",
    "        \n",
    "        print(\"\\n=== Validation Summary ===\")\n",
    "        summarize_validation_results(cleaned_biometric, processed_cask)\n",
    "\n",
    "        cleaned_biometric, processed_cask = filter_to_valid_dates(cleaned_biometric, processed_cask)\n",
    "\n",
    "        # 8. Create Analysis Dataset\n",
    "        print(\"\\n=== Creating Analysis Dataset ===\")\n",
    "        analysis_df = create_analysis_dataset(cleaned_biometric, processed_cask)\n",
    "        if analysis_df is None:\n",
    "            print(\"Error: Failed to create analysis dataset\")\n",
    "            return None, None, None\n",
    "        log_data_shape(analysis_df, \"Combined analysis dataset\")\n",
    "\n",
    "        # 9. Run Analyses\n",
    "        print(\"\\n=== Running Analyses ===\")\n",
    "        try:\n",
    "            print(\"\\nAnalyzing group patterns...\")\n",
    "            pattern_results = analyze_group_patterns(analysis_df)\n",
    "            \n",
    "            print(\"\\nAnalyzing group differences...\")\n",
    "            difference_results = analyze_group_differences(analysis_df)\n",
    "            \n",
    "            print(\"\\nAnalyzing productivity correlations...\")\n",
    "            correlation_results = analyze_productivity_correlations(analysis_df)\n",
    "            \n",
    "            print(\"\\nAnalyzing data quality...\")\n",
    "            quality_results = analyze_data_quality(analysis_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during analysis: {str(e)}\")\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # 10. Generate Visualizations\n",
    "        print(\"\\n=== Generating Visualizations ===\")\n",
    "        try:\n",
    "            daily_plots = create_daily_pattern_plots(pattern_results)\n",
    "            productivity_plots = create_productivity_plots(analysis_df)\n",
    "            correlation_plots = create_correlation_plots(correlation_results)\n",
    "            quality_plots = create_quality_assessment_plots(analysis_df, quality_results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating visualizations: {str(e)}\")\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # 11. Combine Results\n",
    "        analysis_results = {\n",
    "            'patterns': pattern_results,\n",
    "            'differences': difference_results,\n",
    "            'correlations': correlation_results,\n",
    "            'quality': quality_results,\n",
    "            'plots': {\n",
    "                'daily': daily_plots,\n",
    "                'productivity': productivity_plots,\n",
    "                'correlation': correlation_plots,\n",
    "                'quality': quality_plots\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 12. Create Dashboard\n",
    "        print(\"\\n=== Creating Dashboard ===\")\n",
    "        try:\n",
    "            dashboard = create_interactive_dashboard(analysis_df, analysis_results)\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating dashboard: {str(e)}\")\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        # 13. Save Results\n",
    "        print(\"\\n=== Saving Results ===\")\n",
    "        try:\n",
    "            # Create output directories\n",
    "            output_dir = Path('outputs/plots')\n",
    "            output_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Save processed data\n",
    "            analysis_df.to_csv('outputs/processed_analysis_data.csv', index=False)\n",
    "            \n",
    "            # Save plots\n",
    "            for category, plots in analysis_results['plots'].items():\n",
    "                for name, fig in plots.items():\n",
    "                    plot_file = output_dir / f'{category}_{name}.html'\n",
    "                    fig.write_html(str(plot_file))\n",
    "            \n",
    "            # Save dashboard\n",
    "            dashboard.write_html('outputs/dashboard.html')\n",
    "            \n",
    "            # Generate reports\n",
    "            generate_summary_report(analysis_results, analysis_df)\n",
    "            generate_analysis_report(analysis_df, analysis_results, 'outputs/analysis_report.html')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {str(e)}\")\n",
    "            print(f\"Traceback: {traceback.format_exc()}\")\n",
    "            return None, None, None\n",
    "        \n",
    "        print(\"\\n=== Analysis Pipeline Complete! ===\")\n",
    "        print(\"All results saved in 'outputs' directory\")\n",
    "        \n",
    "        return analysis_df, analysis_results, dashboard\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in main execution: {str(e)}\")\n",
    "        print(f\"Traceback: {traceback.format_exc()}\")\n",
    "        return None, None, None\n",
    "\n",
    "def generate_summary_report(analysis_results, analysis_df):\n",
    "    \"\"\"\n",
    "    Generate a summary report of key findings\n",
    "    \"\"\"\n",
    "    with open('outputs/summary_report.txt', 'w') as f:\n",
    "        f.write(\"Analysis Summary Report\\n\")\n",
    "        f.write(\"=====================\\n\\n\")\n",
    "        \n",
    "        # Data coverage\n",
    "        f.write(\"Data Coverage:\\n\")\n",
    "        f.write(\"--------------\\n\")\n",
    "        coverage = analysis_results['quality']['group_coverage']\n",
    "        f.write(f\"{coverage.to_string()}\\n\\n\")\n",
    "        \n",
    "        # Key findings\n",
    "        f.write(\"Key Findings:\\n\")\n",
    "        f.write(\"-------------\\n\")\n",
    "        \n",
    "        # Group differences\n",
    "        differences = analysis_results['differences']\n",
    "        for metric, comparisons in differences.items():\n",
    "            if metric.endswith('_comparisons'):\n",
    "                f.write(f\"\\n{metric.replace('_comparisons', '').title()} Comparisons:\\n\")\n",
    "                for comp in comparisons:\n",
    "                    if comp['p_value'] < 0.05:\n",
    "                        f.write(f\"- Significant difference between {comp['group1']} and {comp['group2']}\\n\")\n",
    "        \n",
    "        # Correlations\n",
    "        f.write(\"\\nStrong Correlations:\\n\")\n",
    "        corr_matrix = analysis_results['correlations']['overall_correlations']\n",
    "        strong_corrs = [(i, j, corr_matrix.loc[i,j]) \n",
    "                       for i in corr_matrix.index \n",
    "                       for j in corr_matrix.columns \n",
    "                       if abs(corr_matrix.loc[i,j]) > 0.7 and i != j]\n",
    "        for i, j, corr in strong_corrs:\n",
    "            f.write(f\"- {i} vs {j}: {corr:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Create outputs directory if it doesn't exist\n",
    "        os.makedirs('outputs/plots', exist_ok=True)\n",
    "        \n",
    "        # Run main analysis\n",
    "        analysis_df, analysis_results, dashboard = main()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
